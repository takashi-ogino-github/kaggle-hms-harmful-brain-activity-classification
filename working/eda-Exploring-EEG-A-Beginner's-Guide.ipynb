{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>1 |</span></b> <b>INTRODUCTION</b></div>\n",
    "\n",
    "👋 Welcome to \"🧠Exploring EEG: A Beginner's Guide\"! \n",
    "\n",
    "If you're fascinated by the wonders of the human brain and the intricate patterns of brainwaves, but find the world of Electroencephalography (EEG) analysis daunting, you're in the right place. \n",
    "\n",
    "This notebook is designed for beginners like me & you, aiming to demystify the complexities of EEG data and make your learning journey both enjoyable and informative.\n",
    "\n",
    "人間の脳の驚異や脳波の複雑なパターンに魅了されているが、脳波 (EEG) 分析の世界には気が遠くなるという方には、ここが正しい場所です。\n",
    "\n",
    "このノートブックは私やあなたのような初心者向けに設計されており、EEG データの複雑さをわかりやすくし、学習を楽しく有益なものにすることを目的としています。\n",
    "\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 1.1 |</span> Intention of the notebook</b>\n",
    "In this notebook, we will embark on an exploratory journey into the realm of EEG data analysis. Our goal is to provide a clear, step-by-step guide to understanding and analyzing EEG signals, which are crucial in detecting and classifying brain activities, such as seizures. We aim to:\n",
    "\n",
    "* Break down complex concepts into easily digestible sections.\n",
    "* Illustrate each step with practical code examples.\n",
    "* Reference public notebooks and discussions to enhance your learning experience.\n",
    "\n",
    "このノートブックでは、EEG データ分析の領域への探索的な旅に乗り出します。 私たちの目標は、発作などの脳活動の検出と分類に重要な EEG 信号を理解し、分析するための明確な段階的なガイドを提供することです。 私たちは次のことを目指しています。\n",
    "\n",
    "* 複雑な概念を理解しやすいセクションに分割します。\n",
    "* 実際のコード例を使用して各ステップを説明します。\n",
    "* 公開ノートやディスカッションを参照して、学習体験を強化します。\n",
    "\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 1.2 |</span> Learning Objective</b>\n",
    "By the end of this notebook, you will have a foundational understanding of:\n",
    "\n",
    "* The basics of EEG signals and their significance in medical research and neurology.\n",
    "* How to preprocess and analyze EEG data.\n",
    "* Run through the basic code to build a machine learning model for EEG data classification.\n",
    "\n",
    "\n",
    "このノートブックを読み終えるまでに、以下についての基礎を理解できるようになります。\n",
    "\n",
    "* EEG 信号の基礎と、医学研究および神経学におけるその重要性。\n",
    "* 脳波データを前処理して分析する方法。\n",
    "* 基本コードを実行して、EEG データ分類のための機械学習モデルを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>2 |</span></b> <b>REFERENCE & ACKNOWLEDGEMENT</b></div>\n",
    "\n",
    "This notebook wouldn't be possible without the valuable insights and contributions from the Kaggle community. I've leveraged several resources to compile the most effective learning path for us:\n",
    "\n",
    "このノートブックは、Kaggle コミュニティからの貴重な洞察と貢献がなければ不可能でした。 私はいくつかのリソースを活用して、最も効果的な学習パスを作成しました。\n",
    "\n",
    "* https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-8\n",
    "* https://www.kaggle.com/code/mvvppp/hms-eda-and-domain-journey\n",
    "* https://www.kaggle.com/code/ksooklall/hms-banana-montage\n",
    "* https://www.kaggle.com/code/mpwolke/seizures-classification-parquet\n",
    "\n",
    "\n",
    "Feel free to explore these resources alongside this notebook to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>3 |</span></b> <b>LOAD LIBARIES</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd, numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "VER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>4 |</span></b> <b>INTRODUCTION TO EEG AND SEIZURE DETECTION</b></div>\n",
    "\n",
    "<b><span style='color:#FFCE30'> 4.1 |</span> Electroencephalography (EEG) - The Window into Brain Activity</b>\n",
    "\n",
    "* Electroencephalography, commonly known as EEG, is a non-invasive method used by medical professionals to record electrical activity in the brain. \n",
    "* This is done using electrodes placed along the scalp. \n",
    "* EEG is a crucial tool in diagnosing neurological disorders, especially epilepsy, which is characterized by recurrent seizures.\n",
    "\n",
    "* 一般にEEGとして知られる脳波検査は、脳内の電気活動を記録するために医療専門家によって使用される非侵襲的な方法です。\n",
    "* 頭皮に沿って電極を設置して行います。\n",
    "* EEG は、神経疾患、特に反復発作を特徴とするてんかんの診断において重要なツールです\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Sebastian-Nagel-4/publication/338423585/figure/fig1/AS:844668573073409@1578396089381/Sketch-of-how-to-record-an-Electroencephalogram-An-EEG-allows-measuring-the-electrical.png\" alt=\"EEG\" width=\"600\" height=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fp1</th>\n",
       "      <th>F3</th>\n",
       "      <th>C3</th>\n",
       "      <th>P3</th>\n",
       "      <th>F7</th>\n",
       "      <th>T3</th>\n",
       "      <th>T5</th>\n",
       "      <th>O1</th>\n",
       "      <th>Fz</th>\n",
       "      <th>Cz</th>\n",
       "      <th>Pz</th>\n",
       "      <th>Fp2</th>\n",
       "      <th>F4</th>\n",
       "      <th>C4</th>\n",
       "      <th>P4</th>\n",
       "      <th>F8</th>\n",
       "      <th>T4</th>\n",
       "      <th>T6</th>\n",
       "      <th>O2</th>\n",
       "      <th>EKG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-105.849998</td>\n",
       "      <td>-89.230003</td>\n",
       "      <td>-79.459999</td>\n",
       "      <td>-49.230000</td>\n",
       "      <td>-99.730003</td>\n",
       "      <td>-87.769997</td>\n",
       "      <td>-53.330002</td>\n",
       "      <td>-50.740002</td>\n",
       "      <td>-32.250000</td>\n",
       "      <td>-42.099998</td>\n",
       "      <td>-43.270000</td>\n",
       "      <td>-88.730003</td>\n",
       "      <td>-74.410004</td>\n",
       "      <td>-92.459999</td>\n",
       "      <td>-58.930000</td>\n",
       "      <td>-75.739998</td>\n",
       "      <td>-59.470001</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>66.489998</td>\n",
       "      <td>1404.930054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-85.470001</td>\n",
       "      <td>-75.070000</td>\n",
       "      <td>-60.259998</td>\n",
       "      <td>-38.919998</td>\n",
       "      <td>-73.080002</td>\n",
       "      <td>-87.510002</td>\n",
       "      <td>-39.680000</td>\n",
       "      <td>-35.630001</td>\n",
       "      <td>-76.839996</td>\n",
       "      <td>-62.740002</td>\n",
       "      <td>-43.040001</td>\n",
       "      <td>-68.629997</td>\n",
       "      <td>-61.689999</td>\n",
       "      <td>-69.320000</td>\n",
       "      <td>-35.790001</td>\n",
       "      <td>-58.900002</td>\n",
       "      <td>-41.660000</td>\n",
       "      <td>196.190002</td>\n",
       "      <td>230.669998</td>\n",
       "      <td>3402.669922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.840000</td>\n",
       "      <td>34.849998</td>\n",
       "      <td>56.430000</td>\n",
       "      <td>67.970001</td>\n",
       "      <td>48.099998</td>\n",
       "      <td>25.350000</td>\n",
       "      <td>80.250000</td>\n",
       "      <td>48.060001</td>\n",
       "      <td>6.720000</td>\n",
       "      <td>37.880001</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>16.580000</td>\n",
       "      <td>55.060001</td>\n",
       "      <td>45.020000</td>\n",
       "      <td>70.529999</td>\n",
       "      <td>47.820000</td>\n",
       "      <td>72.029999</td>\n",
       "      <td>-67.180000</td>\n",
       "      <td>-171.309998</td>\n",
       "      <td>-3565.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-56.320000</td>\n",
       "      <td>-37.279999</td>\n",
       "      <td>-28.100000</td>\n",
       "      <td>-2.820000</td>\n",
       "      <td>-43.430000</td>\n",
       "      <td>-35.049999</td>\n",
       "      <td>3.910000</td>\n",
       "      <td>-12.660000</td>\n",
       "      <td>8.650000</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>-51.900002</td>\n",
       "      <td>-21.889999</td>\n",
       "      <td>-41.330002</td>\n",
       "      <td>-11.580000</td>\n",
       "      <td>-27.040001</td>\n",
       "      <td>-11.730000</td>\n",
       "      <td>-91.000000</td>\n",
       "      <td>-81.190002</td>\n",
       "      <td>-1280.930054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-110.139999</td>\n",
       "      <td>-104.519997</td>\n",
       "      <td>-96.879997</td>\n",
       "      <td>-70.250000</td>\n",
       "      <td>-111.660004</td>\n",
       "      <td>-114.430000</td>\n",
       "      <td>-71.830002</td>\n",
       "      <td>-61.919998</td>\n",
       "      <td>-76.150002</td>\n",
       "      <td>-79.779999</td>\n",
       "      <td>-67.480003</td>\n",
       "      <td>-99.029999</td>\n",
       "      <td>-93.610001</td>\n",
       "      <td>-104.410004</td>\n",
       "      <td>-70.070000</td>\n",
       "      <td>-89.250000</td>\n",
       "      <td>-77.260002</td>\n",
       "      <td>155.729996</td>\n",
       "      <td>264.850006</td>\n",
       "      <td>4325.370117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Fp1          F3         C3         P3          F7          T3  \\\n",
       "0 -105.849998  -89.230003 -79.459999 -49.230000  -99.730003  -87.769997   \n",
       "1  -85.470001  -75.070000 -60.259998 -38.919998  -73.080002  -87.510002   \n",
       "2    8.840000   34.849998  56.430000  67.970001   48.099998   25.350000   \n",
       "3  -56.320000  -37.279999 -28.100000  -2.820000  -43.430000  -35.049999   \n",
       "4 -110.139999 -104.519997 -96.879997 -70.250000 -111.660004 -114.430000   \n",
       "\n",
       "          T5         O1         Fz         Cz         Pz        Fp2  \\\n",
       "0 -53.330002 -50.740002 -32.250000 -42.099998 -43.270000 -88.730003   \n",
       "1 -39.680000 -35.630001 -76.839996 -62.740002 -43.040001 -68.629997   \n",
       "2  80.250000  48.060001   6.720000  37.880001  61.000000  16.580000   \n",
       "3   3.910000 -12.660000   8.650000   3.830000   4.180000 -51.900002   \n",
       "4 -71.830002 -61.919998 -76.150002 -79.779999 -67.480003 -99.029999   \n",
       "\n",
       "          F4          C4         P4         F8         T4          T6  \\\n",
       "0 -74.410004  -92.459999 -58.930000 -75.739998 -59.470001    8.210000   \n",
       "1 -61.689999  -69.320000 -35.790001 -58.900002 -41.660000  196.190002   \n",
       "2  55.060001   45.020000  70.529999  47.820000  72.029999  -67.180000   \n",
       "3 -21.889999  -41.330002 -11.580000 -27.040001 -11.730000  -91.000000   \n",
       "4 -93.610001 -104.410004 -70.070000 -89.250000 -77.260002  155.729996   \n",
       "\n",
       "           O2          EKG  \n",
       "0   66.489998  1404.930054  \n",
       "1  230.669998  3402.669922  \n",
       "2 -171.309998 -3565.800049  \n",
       "3  -81.190002 -1280.930054  \n",
       "4  264.850006  4325.370117  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the reading of one parquet for understanding\n",
    "\n",
    "BASE_PATH = '../input/hms-harmful-brain-activity-classification/'\n",
    "\n",
    "df = pd.DataFrame({'path': glob(BASE_PATH + '**/*.parquet')})\n",
    "\n",
    "df['test_type'] = df['path'].str.split('/').str.get(-2).str.split('_').str.get(-1)\n",
    "df['id'] = df['path'].str.split('/').str.get(-1).str.split('.').str.get(0)\n",
    "\n",
    "df_eeg = pd.read_parquet(BASE_PATH + 'train_eegs/1000913311.parquet')\n",
    "df_eeg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of channels\n",
    "# Assuming each row is a time point and each column is a channel\n",
    "n_channels = df_eeg.shape[1]\n",
    "n_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The headers in the dataset (Fp1, F3, C3, P3, F7, T3, T5, O1, Fz, Cz, Pz, Fp2, F4, C4, P4, F8, T4, T6, O2, EKG) are standard electrode placement labels used in electroencephalography (EEG). \n",
    "* These labels correspond to specific positions on the scalp where EEG electrodes are placed to record brain activity. \n",
    "* Here's a brief overview of what they represent:\n",
    "\n",
    "1. **Fp1, Fp2:** Frontopolar electrodes, located on the forehead, left and right side.\n",
    "2. **F3, F4:** Frontal electrodes, on the left and right side of the forehead.\n",
    "3. **C3, C4:** Central electrodes, placed above the left and right hemispheres of the brain.\n",
    "4. **P3, P4:** Parietal electrodes, located on the upper back portion of the head, left and right sides.\n",
    "5. **O1, O2:** Occipital electrodes, positioned at the back of the head near the visual cortex.\n",
    "6. **T3, T4, T5, T6:** Temporal electrodes, situated on the left and right sides of the head near the ears. They are often involved in monitoring auditory functions.\n",
    "7. **F7, F8:** Frontal-temporal electrodes, located at the front of the temporal lobes.\n",
    "8. **Fz, Cz, Pz:** Midline electrodes, located at the frontal (Fz), central (Cz), and parietal (Pz) positions on the midline of the head.\n",
    "9. **EKG:** Electrocardiogram electrode, which records the heart’s electrical activity. It's not directly related to brain activity but can be important in some EEG analyses.\n",
    "\n",
    "\n",
    "\n",
    "* データセット内のヘッダー (Fp1、F3、C3、P3、F7、T3、T5、O1、Fz、Cz、Pz、Fp2、F4、C4、P4、F8、T4、T6、O2、EKG) は標準電極です 脳波検査 (EEG) で使用される配置ラベル。\n",
    "* これらのラベルは、脳活動を記録するために EEG 電極が配置される頭皮上の特定の位置に対応します。\n",
    "* それらが何を表すかについての簡単な概要は次のとおりです。\n",
    "\n",
    "1. **Fp1、Fp2:** 前頭極電極。額の左側と右側にあります。\n",
    "2. **F3、F4:** 額の左側と右側にある前頭部電極。\n",
    "3. **C3、C4:** 中心電極。脳の左半球と右半球の上に配置されます。\n",
    "4. **P3、P4:** 頭頂部電極。頭の後部上部の左側と右側にあります。\n",
    "5. **O1、O2:** 後頭電極。後頭部の視覚野近くに配置されます。\n",
    "6. **T3、T4、T5、T6:** 側頭電極。耳の近くの頭の左側と右側にあります。 彼らは多くの場合、聴覚機能の監視に関与します。\n",
    "7. **F7、F8:** 側頭葉の前部に位置する前頭側頭電極。\n",
    "8. **Fz、Cz、Pz:** 正中線電極。頭の正中線上の前頭部 (Fz)、中央 (Cz)、および頭頂部 (Pz) の位置にあります。\n",
    "9. **EKG:** 心臓の電気活動を記録する心電図電極。 これは脳の活動とは直接関係しませんが、一部の EEG 分析では重要になる可能性があります。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Danny-Plass-Oude-Bos/publication/237777779/figure/fig3/AS:669556259434497@1536646060035/10-20-system-of-electrode-placement.png\" alt=\"10-20-system-of-electrode-placement\" width=\"300\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style='color:#FFCE30'> 4.2 |</span> Seizures and Their Impact</b>\n",
    "* Seizures are sudden, uncontrolled electrical disturbances in the brain that can cause changes in behavior, feelings, movements, and levels of consciousness. \n",
    "* Detecting and classifying seizures accurately is vital for appropriate treatment and care, especially in critically ill patients.\n",
    "\n",
    "\n",
    "* 発作は、行動、感情、動き、意識レベルの変化を引き起こす可能性がある、脳内の突然の制御不能な電気的障害です。\n",
    "* 発作を正確に検出して分類することは、特に重症患者の場合、適切な治療とケアに不可欠です。\n",
    "\n",
    "\n",
    "<b><span style='color:#FFCE30'> 4.3 |</span> The Challenge of Manual EEG Analysis</b>\n",
    "\n",
    "* Traditionally, EEG data analysis relies on visual inspection by trained neurologists. \n",
    "* This process is not only time-consuming and labor-intensive but also prone to errors due to fatigue and subjective interpretation.\n",
    "\n",
    "\n",
    "* 従来、EEG データ分析は訓練を受けた神経内科医による目視検査に依存していました。\n",
    "* このプロセスは時間と労力がかかるだけでなく、疲労や主観的な解釈によりエラーが発生しやすくなります。\n",
    "\n",
    "<img src=\"https://slideplayer.com/slide/12925171/78/images/2/Manual+Interpretation+of+EEGs.jpg\" alt=\"Manual Interpretation of EEG\" width=\"700\" height=\"300\">\n",
    "Source: Automated Identification of Abnormal Adult EEG, S. López, G. Suarez, D. Jungreis, I. Obeid and J. Picone, Neural Engineering Data Consortium, Temple University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style='color:#FFCE30'> 4.4 |</span> The Role of Data Science in EEG Analysis</b>\n",
    "\n",
    "* Automating EEG Interpretation\n",
    "The advent of machine learning and data science offers an opportunity to automate the interpretation of EEG data. By developing algorithms that can detect and classify different patterns in EEG signals, we can aid neurologists in making faster, more accurate diagnoses.\n",
    "\n",
    "* The Data Science Approach\n",
    "Data scientists approach this challenge by first preprocessing the EEG data, which involves filtering out noise and extracting relevant features. Machine learning models are then trained on these features to distinguish between different types of brain activity.\n",
    "\n",
    "\n",
    "* EEG解釈の自動化\n",
    "機械学習とデータ サイエンスの出現により、EEG データの解釈を自動化する機会が生まれました。 EEG信号のさまざまなパターンを検出して分類できるアルゴリズムを開発することで、神経内科医がより迅速かつ正確な診断を行えるように支援できます。\n",
    "\n",
    "* データサイエンスのアプローチ\n",
    "データ サイエンティストは、まず EEG データを前処理することでこの課題に取り組みます。これには、ノイズのフィルタリングと関連する特徴の抽出が含まれます。 次に、機械学習モデルはこれらの特徴に基づいてトレーニングされ、さまざまな種類の脳活動を区別します。\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Huiguang-He/publication/336336651/figure/fig1/AS:834361356197888@1575938657076/The-flow-chart-of-EEG-emotion-classification-with-similarity-learning-network.png\" alt=\"flowchart for EEG classification\" width=\"700\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style='color:#FFCE30'> 4.5 |</span> Understanding EEG Patterns</b>\n",
    "\n",
    "In the realm of EEG analysis for seizure detection, certain patterns are of particular interest:\n",
    "\n",
    "1. **Seizure (SZ):** Characterized by abnormal rhythmic activity, indicative of a seizure.\n",
    "2. **Generalized Periodic Discharges (GPD):** Patterns that may be seen in various encephalopathies.\n",
    "3. **Lateralized Periodic Discharges (LPD):** Often associated with focal brain lesions.\n",
    "4. **Lateralized Rhythmic Delta Activity (LRDA):** Can be observed in focal brain dysfunction.\n",
    "5. **Generalized Rhythmic Delta Activity (GRDA):** Typically related to diffuse brain dysfunction.\n",
    "6. **\"Other\" Patterns:** Any other type of activity not falling into the above categories.\n",
    "\n",
    "\n",
    "発作検出のための EEG 解析の分野では、特定のパターンが特に重要です。\n",
    "\n",
    "1. **発作 (SZ):** 発作を示す異常なリズム活動を特徴とします。\n",
    "2. **全般性周期放電 (GPD):** さまざまな脳症で見られるパターン。\n",
    "3. **側方化周期放電 (LPD):** 多くの場合、限局性脳病変に関連します。\n",
    "4. **側方化リズムデルタ活動 (LRDA):** 局所性脳機能障害で観察される可能性があります。\n",
    "5. **一般化リズムデルタ活動 (GRDA):** 通常、びまん性脳機能障害に関連します。\n",
    "6. **「その他」パターン:** 上記のカテゴリに当てはまらないその他の種類のアクティビティ。\n",
    "\n",
    "\n",
    "<b><span style='color:#FFCE30'> 4.6 |</span> Interpreting Complex EEG Data</b>\n",
    "\n",
    "EEG data interpretation can be complex, especially in edge cases where expert neurologists may not agree on a classification. This is where machine learning models can particularly shine by providing an additional layer of analysis.\n",
    "\n",
    "\n",
    "EEG データの解釈は、特に専門の神経内科医が分類に同意しない可能性がある特殊なケースでは、複雑になる可能性があります。 ここでは、追加の分析レイヤーを提供することで、機械学習モデルが特に威力を発揮します。\n",
    "\n",
    "\n",
    "<img src=\"https://www.neurology.org/cms/10.1212/WNL.0000000000207127/asset/bd84c182-712c-41ab-8742-cecf9d49a322/assets/images/large/5ff2.jpg\" alt=\"flowchart for EEG classification\" width=\"700\" height=\"300\">\n",
    "\n",
    "Source: Development of Expert-Level Classification of Seizures and Rhythmic and Periodic Patterns During EEG Interpretation https://www.neurology.org/doi/10.1212/WNL.0000000000207127\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>5 |</span></b> <b>LOAD TRAIN DATA</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (106800, 15)\n",
      "Targets ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>eeg_sub_id</th>\n",
       "      <th>eeg_label_offset_seconds</th>\n",
       "      <th>spectrogram_id</th>\n",
       "      <th>spectrogram_sub_id</th>\n",
       "      <th>spectrogram_label_offset_seconds</th>\n",
       "      <th>label_id</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>expert_consensus</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>127492639</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3887563113</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1142670488</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2718991173</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1628180742</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>353733</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3080632009</td>\n",
       "      <td>42516</td>\n",
       "      <td>Seizure</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
       "0  1628180742           0                       0.0          353733   \n",
       "1  1628180742           1                       6.0          353733   \n",
       "2  1628180742           2                       8.0          353733   \n",
       "3  1628180742           3                      18.0          353733   \n",
       "4  1628180742           4                      24.0          353733   \n",
       "\n",
       "   spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
       "0                   0                               0.0   127492639   \n",
       "1                   1                               6.0  3887563113   \n",
       "2                   2                               8.0  1142670488   \n",
       "3                   3                              18.0  2718991173   \n",
       "4                   4                              24.0  3080632009   \n",
       "\n",
       "   patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n",
       "0       42516          Seizure             3         0         0          0   \n",
       "1       42516          Seizure             3         0         0          0   \n",
       "2       42516          Seizure             3         0         0          0   \n",
       "3       42516          Seizure             3         0         0          0   \n",
       "4       42516          Seizure             3         0         0          0   \n",
       "\n",
       "   grda_vote  other_vote  \n",
       "0          0           0  \n",
       "1          0           0  \n",
       "2          0           0  \n",
       "3          0           0  \n",
       "4          0           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('..//input/hms-harmful-brain-activity-classification/train.csv')\n",
    "TARGETS = df.columns[-6:]\n",
    "print('Train shape:', df.shape )\n",
    "print('Targets', list(TARGETS))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>6 |</span></b> <b>CREATE NON-OVERLAPPING EEG ID TRAIN DATA</b></div>\n",
    "\n",
    "Following the notebook from Chris Deotte: https://www.kaggle.com/code/cdeotte/catboost-starter-lb-0-8,\n",
    "Initial discussion found here https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/467021\n",
    "\n",
    "We perform the following because:\n",
    "\n",
    "* **Match Training Data with Test Data Format:** The competition states that the test data does not have multiple segments from the same eeg_id. To make the training data similar to the test data, we also use only one segment per eeg_id in the training data.\n",
    "\n",
    "* **Remove Redundancies:** This approach ensures that the training data does not have overlapping or redundant information, which can lead to a more accurate and generalizable machine learning model.\n",
    "\n",
    "* **Consistency in Data:** By standardizing how we handle the EEG segments in training, we ensure that our model learns from data that is consistent in format with the data it will be tested on.\n",
    "\n",
    "* **Data Preparation for Machine Learning:** The normalization of target variables and inclusion of relevant features like patient_id and expert_consensus prepare the dataset for effective machine learning modeling.\n",
    "\n",
    "\n",
    "次の理由から、次のことを実行します。\n",
    "\n",
    "* **トレーニング データをテスト データ形式と一致させる:** コンテストでは、テスト データには同じ eeg_id からの複数のセグメントが含まれていないと記載されています。 トレーニング データをテスト データと同様にするために、トレーニング データの eeg_id ごとに 1 つのセグメントのみを使用します。\n",
    "\n",
    "* **冗長性の削除:** このアプローチにより、トレーニング データに重複または冗長な情報が含まれないことが保証され、より正確で一般化可能な機械学習モデルが得られます。\n",
    "\n",
    "* **データの一貫性:** トレーニングでの EEG セグメントの処理方法を標準化することで、テスト対象のデータと形式が一貫しているデータからモデルが学習することを保証します。\n",
    "\n",
    "* **機械学習のためのデータ準備:** ターゲット変数の正規化と、patient_id や Expert_consensus などの関連機能の組み込みにより、効果的な機械学習モデリングのためのデータセットが準備されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train non-overlapp eeg_id shape: (17089, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>other_vote</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>LPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>5955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>38549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>GPD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eeg_id     spec_id     min     max  patient_id  seizure_vote  lpd_vote  \\\n",
       "0  568657   789577333     0.0    16.0       20654           0.0  0.000000   \n",
       "1  582999  1552638400     0.0    38.0       20230           0.0  0.857143   \n",
       "2  642382    14960202  1008.0  1032.0        5955           0.0  0.000000   \n",
       "3  751790   618728447   908.0   908.0       38549           0.0  0.000000   \n",
       "4  778705    52296320     0.0     0.0       40955           0.0  0.000000   \n",
       "\n",
       "   gpd_vote  lrda_vote  grda_vote  other_vote target  \n",
       "0      0.25   0.000000   0.166667    0.583333  Other  \n",
       "1      0.00   0.071429   0.000000    0.071429    LPD  \n",
       "2      0.00   0.000000   0.000000    1.000000  Other  \n",
       "3      1.00   0.000000   0.000000    0.000000    GPD  \n",
       "4      0.00   0.000000   0.000000    1.000000  Other  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Unique EEG Segment per eeg_id:\n",
    "# The code groups (groupby) the EEG data (df) by eeg_id. Each eeg_id represents a different EEG recording.\n",
    "# It then picks the first spectrogram_id and the earliest (min) spectrogram_label_offset_seconds for each eeg_id. This helps in identifying the starting point of each EEG segment.\n",
    "# The resulting DataFrame train has columns spec_id (first spectrogram_id) and min (earliest spectrogram_label_offset_seconds).\n",
    "\n",
    "# eeg_id ごとに一意の EEG セグメントを作成:\n",
    "# このコードは、eeg_id によって EEG データ (df) をグループ化 (groupby) します。 各 eeg_id は異なる EEG 記録を表します。\n",
    "# 次に、各 eeg_id の最初の spectrogram_id と最も早い (最小) spectrogram_label_offset_seconds を選択します。 これは、各 EEG セグメントの開始点を特定するのに役立ちます。\n",
    "# 結果として得られる DataFrame トレインには、列 spec_id (最初の spectrogram_id) と min (最も古い spectrogram_label_offset_seconds) があります。\n",
    "\n",
    "train = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n",
    "    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\n",
    "train.columns = ['spec_id','min']\n",
    "\n",
    "\n",
    "# Finding the Latest Point in Each EEG Segment:\n",
    "# The code again groups the data by eeg_id and finds the latest (max) spectrogram_label_offset_seconds for each segment.\n",
    "# This max value is added to the train DataFrame, representing the end point of each EEG segment.\n",
    "\n",
    "# 各 EEG セグメントの最新ポイントを見つける:\n",
    "# コードは再び eeg_id によってデータをグループ化し、各セグメントの最新 (最大) spectrogram_label_offset_seconds を見つけます。\n",
    "# この最大値はトレイン データフレームに追加され、各 EEG セグメントの終了点を表します。\n",
    "\n",
    "tmp = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n",
    "    {'spectrogram_label_offset_seconds':'max'})\n",
    "train['max'] = tmp\n",
    "\n",
    "# このコードは、各 eeg_id のpatient_id をトレイン DataFrame に追加します。 これにより、各 EEG セグメントが特定の患者に関連付けられます。\n",
    "tmp = df.groupby('eeg_id')[['patient_id']].agg('first') # The code adds the patient_id for each eeg_id to the train DataFrame. This links each EEG segment to a specific patient.\n",
    "train['patient_id'] = tmp\n",
    "\n",
    "# コードは、各 eeg_id のターゲット変数カウント (発作、LPD などの投票など) を合計します。\n",
    "tmp = df.groupby('eeg_id')[TARGETS].agg('sum') # The code sums up the target variable counts (like votes for seizure, LPD, etc.) for each eeg_id.\n",
    "for t in TARGETS:\n",
    "    train[t] = tmp[t].values\n",
    "\n",
    "# その後、合計が 1 になるようにこれらのカウントを正規化します。このステップでは、カウントを確率に変換します。これは、分類タスクでは一般的な方法です。    \n",
    "y_data = train[TARGETS].values # It then normalizes these counts so that they sum up to 1. This step converts the counts into probabilities, which is a common practice in classification tasks.\n",
    "y_data = y_data / y_data.sum(axis=1,keepdims=True)\n",
    "train[TARGETS] = y_data\n",
    "\n",
    "# 各 eeg_id について、コードには EEG セグメントの分類に関する Expert_consensus が含まれます。\n",
    "tmp = df.groupby('eeg_id')[['expert_consensus']].agg('first') # For each eeg_id, the code includes the expert_consensus on the EEG segment's classification.\n",
    "train['target'] = tmp\n",
    "\n",
    "# これにより、eeg_id が通常の列になり、DataFrame の操作が容易になります。\n",
    "train = train.reset_index() # This makes eeg_id a regular column, making the DataFrame easier to work with.\n",
    "print('Train non-overlapp eeg_id shape:', train.shape )\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>7 |</span></b> <b>FEATURE ENGINEERING</b></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style='color:#FFCE30'> 7.1 |</span> 10 min and 20 sec windows</b>\n",
    "\n",
    "* The code belows efficiently reads spectrogram data, from a single combined file, based on the set variable. We relied on the dataset by Chris Deotte to save time. https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
    "* It then performs feature engineering by calculating mean and minimum values over two different time windows for each frequency in the spectrogram.\n",
    "It produce produces in 1600 features (400 features × 4 calculations) for each EEG ID.\n",
    "* The new features are intended to help the model better understand and classify the EEG data.\n",
    "* This approach is designed to enhance the model's performance by providing it with more detailed information derived from the spectrogram data.\n",
    "\n",
    "\n",
    "* 以下のコードは、設定された変数に基づいて、単一の結合されたファイルからスペクトログラム データを効率的に読み取ります。 時間を節約するために、Chris Deotte によるデータセットに依存しました。 https://www.kaggle.com/datasets/cdeotte/brain-spectrograms\n",
    "* 次に、スペクトログラム内の各周波数の 2 つの異なる時間ウィンドウにわたって平均値と最小値を計算することにより、特徴エンジニアリングを実行します。\n",
    "EEG ID ごとに 1600 個の特徴 (400 個の特徴 × 4 回の計算) が生成されます。\n",
    "* 新しい機能は、モデルが EEG データをよりよく理解して分類できるようにすることを目的としています。\n",
    "* このアプローチは、スペクトログラム データから得られるより詳細な情報をモデルに提供することでモデルのパフォーマンスを向上させるように設計されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_SPEC_FILES = False # If READ_SPEC_FILES is False, the code reads the combined file instead of individual files.\n",
    "FEATURE_ENGINEER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11138 spectrogram parquets\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# READ ALL SPECTROGRAMS\n",
    "PATH = '../input/hms-harmful-brain-activity-classification/train_spectrograms/'\n",
    "files = os.listdir(PATH)\n",
    "print(f'There are {len(files)} spectrogram parquets')\n",
    "\n",
    "if READ_SPEC_FILES:    \n",
    "    spectrograms = {}\n",
    "    for i,f in enumerate(files):\n",
    "        if i%100==0: print(i,', ',end='')\n",
    "        tmp = pd.read_parquet(f'{PATH}{f}')\n",
    "        name = int(f.split('.')[0])\n",
    "        spectrograms[name] = tmp.iloc[:,1:].values\n",
    "else:\n",
    "    spectrograms = np.load('../input/hms-harmful-brain-activity-classification/brain-spectrograms/specs.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "We are creating 1600 features for 17089 rows... 0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , 10000 , 10100 , 10200 , 10300 , 10400 , 10500 , 10600 , 10700 , 10800 , 10900 , 11000 , 11100 , 11200 , 11300 , 11400 , 11500 , 11600 , 11700 , 11800 , 11900 , 12000 , 12100 , 12200 , 12300 , 12400 , 12500 , 12600 , 12700 , 12800 , 12900 , 13000 , 13100 , 13200 , 13300 , 13400 , 13500 , 13600 , 13700 , 13800 , 13900 , 14000 , 14100 , 14200 , 14300 , 14400 , 14500 , 14600 , 14700 , 14800 , 14900 , 15000 , 15100 , 15200 , 15300 , 15400 , 15500 , 15600 , 15700 , 15800 , 15900 , 16000 , 16100 , 16200 , 16300 , 16400 , 16500 , 16600 , 16700 , 16800 , 16900 , 17000 , \n",
      "New train shape: (17089, 1612)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# ENGINEER FEATURES\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The code generates features from the spectrogram data for use in a model \n",
    "# The features are derived by calculating the mean and minimum values over time for each of the 400 spectrogram frequencies.\n",
    "# Two types of windows are used for these calculations:\n",
    "# A 10-minute window (_mean_10m, _min_10m).\n",
    "# A 20-second window (_mean_20s, _min_20s).\n",
    "# This process results in 1600 features (400 features × 4 calculations) for each EEG ID.\n",
    "\n",
    "# このコードは、モデルで使用するためにスペクトログラム データから特徴を生成します。\n",
    "# 特徴は、400 のスペクトログラム周波数ごとに経時的な平均値と最小値を計算することによって導出されます。\n",
    "# これらの計算には 2 種類のウィンドウが使用されます。\n",
    "# 10 分のウィンドウ (_mean_10m、_min_10m)。\n",
    "# 20 秒のウィンドウ (_mean_20s、_min_20s)。\n",
    "# このプロセスにより、EEG ID ごとに 1600 個の特徴 (400 個の特徴 × 4 回の計算) が生成されます。\n",
    "\n",
    "\n",
    "SPEC_COLS = pd.read_parquet(f'{PATH}1000086677.parquet').columns[1:]\n",
    "FEATURES = [f'{c}_mean_10m' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_min_10m' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_mean_20s' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_min_20s' for c in SPEC_COLS]\n",
    "print(f'We are creating {len(FEATURES)} features for {len(train)} rows... ',end='')\n",
    "\n",
    "\n",
    "# A data matrix data is initialized to store the new features for each eeg_id in the train DataFrame.\n",
    "# For each row in train, the code calculates the mean and minimum values within the specified 10-minute and 20-second windows.\n",
    "# These calculated values are then stored in the data matrix.\n",
    "# Finally, the matrix is added to the train DataFrame as new columns.\n",
    "\n",
    "# データ行列データは、トレイン データフレーム内の各 eeg_id の新しい特徴を格納するために初期化されます。\n",
    "# トレイン内の各行について、コードは指定された 10 分および 20 秒のウィンドウ内の平均値と最小値を計算します。\n",
    "# これらの計算された値はデータ マトリックスに保存されます。\n",
    "# 最後に、行列が新しい列としてトレイン データフレームに追加されます。\n",
    "\n",
    "if FEATURE_ENGINEER:\n",
    "    data = np.zeros((len(train),len(FEATURES)))\n",
    "    for k in range(len(train)):\n",
    "        if k%100==0: print(k,', ',end='')\n",
    "        row = train.iloc[k]\n",
    "        r = int( (row['min'] + row['max'])//4 ) \n",
    "        \n",
    "        # 10 MINUTE WINDOW FEATURES (MEANS and MINS)\n",
    "        x = np.nanmean(spectrograms[row.spec_id][r:r+300,:],axis=0)\n",
    "        data[k,:400] = x\n",
    "        x = np.nanmin(spectrograms[row.spec_id][r:r+300,:],axis=0)\n",
    "        data[k,400:800] = x\n",
    "        \n",
    "        # 20 SECOND WINDOW FEATURES (MEANS and MINS)\n",
    "        x = np.nanmean(spectrograms[row.spec_id][r+145:r+155,:],axis=0)\n",
    "        data[k,800:1200] = x\n",
    "        x = np.nanmin(spectrograms[row.spec_id][r+145:r+155,:],axis=0)\n",
    "        data[k,1200:1600] = x\n",
    "\n",
    "    train[FEATURES] = data\n",
    "else:\n",
    "    train = pd.read_parquet('../input/hms-harmful-brain-activity-classification/brain-spectrograms/train.pqt')\n",
    "print()\n",
    "print('New train shape:',train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><span style='color:#FFCE30'> 7.2 |</span>  Frequency Band Analysis</b>\n",
    "\n",
    "#### Frequency Band Feature Extraction:\n",
    "\n",
    "* The function extract_frequency_band_features is designed to process a segment of EEG data. EEG data is a complex signal that represents the electrical activity of the brain.\n",
    "* This function divides the EEG signal into different frequency bands: Delta, Theta, Alpha, Beta, and Gamma. These bands are significant in neuroscientific studies as they are associated with different brain states and activities.\n",
    "\n",
    "\n",
    "* 関数 extract_frequency_band_features は、EEG データのセグメントを処理するように設計されています。 EEG データは、脳の電気活動を表す複雑な信号です。\n",
    "* この機能は、EEG 信号をさまざまな周波数帯域 (デルタ、シータ、アルファ、ベータ、ガンマ) に分割します。 これらのバンドは、さまざまな脳の状態や活動に関連しているため、神経科学研究において重要です。\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/3-s2.0-B9780128044902000026-f02-01-9780128044902.jpg)\n",
    "\n",
    "\n",
    "1. **Delta (0.5 – 4 Hz):**\n",
    "Delta waves are the slowest brainwaves and are typically associated with deep sleep and restorative processes in the body. They are most prominent during dreamless sleep and play a role in healing and regeneration.\n",
    "2. **Theta (4 – 8 Hz):**\n",
    "Theta waves occur during light sleep, deep meditation, and REM (Rapid Eye Movement) sleep. They are linked to creativity, intuition, daydreaming, and fantasizing. Theta states are often associated with subconscious mind activities.\n",
    "3. **Alpha (8 – 12 Hz):**\n",
    "Alpha waves are present during physically and mentally relaxed states but still alert. They are typical in wakeful states that involve a relaxed and effortless alertness. Alpha waves aid in mental coordination, calmness, alertness, and learning.\n",
    "4. **Beta (12 – 30 Hz):**\n",
    "Beta waves dominate our normal waking state of consciousness when attention is directed towards cognitive tasks and the outside world. They are associated with active, busy or anxious thinking and active concentration.\n",
    "5. **Gamma (30 – 45 Hz):**\n",
    "Gamma waves are involved in higher mental activity and consolidation of information. They are important for learning, memory, and information processing. Gamma waves are thought to be the fastest brainwave frequency and relate to simultaneous processing of information from different brain areas.\n",
    "\n",
    "- 各周波数帯域\n",
    "\n",
    "\n",
    "1. **デルタ (0.5 – 4 Hz):**\n",
    "デルタ波は最も遅い脳波であり、通常は深い睡眠と体内の回復プロセスに関連しています。 それらは夢のない睡眠中に最も顕著であり、治癒と再生に役割を果たします。\n",
    "2. **シータ (4 – 8 Hz):**\n",
    "シータ波は、浅い睡眠、深い瞑想、レム睡眠（急速眼球運動）中に発生します。 それらは創造性、直感、空想、空想と結びついています。 シータ状態は、多くの場合、潜在意識の活動に関連しています。\n",
    "3. **アルファ (8 – 12 Hz):**\n",
    "アルファ波は、肉体的および精神的にリラックスしている状態でも、まだ警戒しているときに存在します。 これらは、リラックスした楽な覚醒状態を伴う覚醒状態に典型的に見られます。 アルファ波は、精神的な調整、落ち着き、注意力、学習を助けます。\n",
    "4. **ベータ (12 – 30 Hz):**\n",
    "認知作業や外界に注意が向けられているとき、ベータ波は通常の覚醒意識状態を支配します。 これらは、活動的、多忙、または不安な思考と活発な集中力に関連しています。\n",
    "5. **ガンマ (30 – 45 Hz):**\n",
    "ガンマ波は高次の精神活動と情報の統合に関与します。 これらは学習、記憶、情報処理にとって重要です。 ガンマ波は最も速い脳波周波数であると考えられており、脳のさまざまな領域からの情報の同時処理に関係しています。\n",
    "\n",
    "* For each frequency band, the function applies a bandpass filter to isolate that band's signal. It then computes statistical features (mean, standard deviation, maximum, and minimum) for each band, effectively capturing the characteristics of the EEG signal in these different frequency ranges.\n",
    "* The use of np.nanmean, np.nanstd, np.nanmax, and np.nanmin ensures that the calculations are robust to NaN (Not a Number) values in the data, which might occur due to various reasons like signal loss or artifacts.\n",
    "\n",
    "* 各周波数帯域に対して、この関数はバンドパス フィルターを適用して、その帯域の信号を分離します。 次に、各帯域の統計的特徴 (平均、標準偏差、最大、最小) を計算し、これらの異なる周波数範囲の EEG 信号の特性を効果的に捕捉します。\n",
    "* np.nanmean、np.nanstd、np.nanmax、および np.nanmin を使用すると、信号損失やアーティファクトなどのさまざまな理由によって発生する可能性のあるデータ内の NaN (非数値) 値に対して計算が確実に堅牢になります。\n",
    "\n",
    "\n",
    "#### Feature Aggregation and PCA:\n",
    "\n",
    "* The main script initializes a Principal Component Analysis (PCA) model with the intention of reducing the dimensionality of the extracted features. PCA is a common technique used to transform high-dimensional datasets into a lower-dimensional space while retaining most of the variance in the data.\n",
    "* The script iterates over rows in the train dataset, extracting EEG segments and applying the extract_frequency_band_features function to each channel in these segments. The extracted features from all channels are aggregated.\n",
    "* However, before applying PCA, any NaN values in the aggregated data (data_original) are handled using mean imputation. This step ensures that the PCA algorithm, which cannot handle NaN values, receives a clean dataset.\n",
    "* After imputation, PCA is applied to transform the features into a principal component space, and these transformed features are added back into the train DataFrame.\n",
    "* This process ultimately results in a feature set that's potentially more informative and concise for machine learning models, helping in tasks like classification or anomaly detection in EEG data.\n",
    "\n",
    "\n",
    "* メイン スクリプトは、抽出された特徴の次元を削減することを目的として、主成分分析 (PCA) モデルを初期化します。 PCA は、データ内の分散の大部分を保持しながら、高次元のデータセットを低次元の空間に変換するために使用される一般的な手法です。\n",
    "* スクリプトはトレイン データセット内の行を繰り返し、EEG セグメントを抽出し、これらのセグメント内の各チャネルに extract_frequency_band_features 関数を適用します。 すべてのチャネルから抽出された特徴が集約されます。\n",
    "* ただし、PCA を適用する前に、集計データ (data_original) 内の NaN 値は平均値補完を使用して処理されます。 このステップにより、NaN 値を処理できない PCA アルゴリズムがクリーンなデータセットを受け取ることが保証されます。\n",
    "* 代入後、PCA を適用して特徴を主成分空間に変換し、これらの変換された特徴をトレイン データフレームに追加し直します。\n",
    "* このプロセスにより、最終的に機械学習モデルにとってより有益で簡潔な機能セットが生成され、EEG データの分類や異常検出などのタスクに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_band_features(segment):\n",
    "    # Define EEG frequency bands\n",
    "    eeg_bands = {'Delta': (0.5, 4), 'Theta': (4, 8), 'Alpha': (8, 12), 'Beta': (12, 30), 'Gamma': (30, 45)}\n",
    "    \n",
    "    band_features = []\n",
    "    for band in eeg_bands:\n",
    "        low, high = eeg_bands[band]\n",
    "        # Filter signal for the specific band\n",
    "        band_pass_filter = signal.butter(3, [low, high], btype='bandpass', fs=200, output='sos')\n",
    "        filtered = signal.sosfilt(band_pass_filter, segment)\n",
    "        # Extract features like mean, standard deviation, etc.\n",
    "        band_features.extend([np.nanmean(filtered), np.nanstd(filtered), np.nanmax(filtered), np.nanmin(filtered)])\n",
    "    \n",
    "    return band_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eeg_id</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>seizure_vote</th>\n",
       "      <th>lpd_vote</th>\n",
       "      <th>gpd_vote</th>\n",
       "      <th>lrda_vote</th>\n",
       "      <th>grda_vote</th>\n",
       "      <th>...</th>\n",
       "      <th>RP_18.16_min_20s</th>\n",
       "      <th>RP_18.36_min_20s</th>\n",
       "      <th>RP_18.55_min_20s</th>\n",
       "      <th>RP_18.75_min_20s</th>\n",
       "      <th>RP_18.95_min_20s</th>\n",
       "      <th>RP_19.14_min_20s</th>\n",
       "      <th>RP_19.34_min_20s</th>\n",
       "      <th>RP_19.53_min_20s</th>\n",
       "      <th>RP_19.73_min_20s</th>\n",
       "      <th>RP_19.92_min_20s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568657</td>\n",
       "      <td>789577333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20654</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034223</td>\n",
       "      <td>-0.034364</td>\n",
       "      <td>-0.033143</td>\n",
       "      <td>-0.033080</td>\n",
       "      <td>-0.031165</td>\n",
       "      <td>-0.025752</td>\n",
       "      <td>-0.026022</td>\n",
       "      <td>-0.026978</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>-0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>582999</td>\n",
       "      <td>1552638400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034279</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>-0.033181</td>\n",
       "      <td>-0.033113</td>\n",
       "      <td>-0.031205</td>\n",
       "      <td>-0.025768</td>\n",
       "      <td>-0.026030</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>642382</td>\n",
       "      <td>14960202</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>5955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034270</td>\n",
       "      <td>-0.034411</td>\n",
       "      <td>-0.033168</td>\n",
       "      <td>-0.033106</td>\n",
       "      <td>-0.031201</td>\n",
       "      <td>-0.025765</td>\n",
       "      <td>-0.026030</td>\n",
       "      <td>-0.026981</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>751790</td>\n",
       "      <td>618728447</td>\n",
       "      <td>908.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>38549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034267</td>\n",
       "      <td>-0.034408</td>\n",
       "      <td>-0.033173</td>\n",
       "      <td>-0.033103</td>\n",
       "      <td>-0.031198</td>\n",
       "      <td>-0.025765</td>\n",
       "      <td>-0.026027</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>778705</td>\n",
       "      <td>52296320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034239</td>\n",
       "      <td>-0.034376</td>\n",
       "      <td>-0.033153</td>\n",
       "      <td>-0.033087</td>\n",
       "      <td>-0.031194</td>\n",
       "      <td>-0.025765</td>\n",
       "      <td>-0.026028</td>\n",
       "      <td>-0.026980</td>\n",
       "      <td>-0.024814</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17084</th>\n",
       "      <td>4293354003</td>\n",
       "      <td>1188113564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034282</td>\n",
       "      <td>-0.034420</td>\n",
       "      <td>-0.033183</td>\n",
       "      <td>-0.033115</td>\n",
       "      <td>-0.031206</td>\n",
       "      <td>-0.025769</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17085</th>\n",
       "      <td>4293843368</td>\n",
       "      <td>1549502620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034214</td>\n",
       "      <td>-0.034326</td>\n",
       "      <td>-0.033102</td>\n",
       "      <td>-0.033006</td>\n",
       "      <td>-0.031130</td>\n",
       "      <td>-0.025725</td>\n",
       "      <td>-0.026012</td>\n",
       "      <td>-0.026975</td>\n",
       "      <td>-0.024812</td>\n",
       "      <td>-0.026098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17086</th>\n",
       "      <td>4294455489</td>\n",
       "      <td>2105480289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034285</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>-0.033186</td>\n",
       "      <td>-0.033117</td>\n",
       "      <td>-0.031208</td>\n",
       "      <td>-0.025770</td>\n",
       "      <td>-0.026032</td>\n",
       "      <td>-0.026983</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>4294858825</td>\n",
       "      <td>657299228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034279</td>\n",
       "      <td>-0.034417</td>\n",
       "      <td>-0.033178</td>\n",
       "      <td>-0.033110</td>\n",
       "      <td>-0.031201</td>\n",
       "      <td>-0.025767</td>\n",
       "      <td>-0.026030</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>4294958358</td>\n",
       "      <td>260520016</td>\n",
       "      <td>2508.0</td>\n",
       "      <td>2508.0</td>\n",
       "      <td>25986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034282</td>\n",
       "      <td>-0.034420</td>\n",
       "      <td>-0.033183</td>\n",
       "      <td>-0.033113</td>\n",
       "      <td>-0.031205</td>\n",
       "      <td>-0.025768</td>\n",
       "      <td>-0.026031</td>\n",
       "      <td>-0.026982</td>\n",
       "      <td>-0.024815</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17089 rows × 1612 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           eeg_id     spec_id     min     max  patient_id  seizure_vote  \\\n",
       "0          568657   789577333     0.0    16.0       20654           0.0   \n",
       "1          582999  1552638400     0.0    38.0       20230           0.0   \n",
       "2          642382    14960202  1008.0  1032.0        5955           0.0   \n",
       "3          751790   618728447   908.0   908.0       38549           0.0   \n",
       "4          778705    52296320     0.0     0.0       40955           0.0   \n",
       "...           ...         ...     ...     ...         ...           ...   \n",
       "17084  4293354003  1188113564     0.0     0.0       16610           0.0   \n",
       "17085  4293843368  1549502620     0.0     0.0       15065           0.0   \n",
       "17086  4294455489  2105480289     0.0     0.0          56           0.0   \n",
       "17087  4294858825   657299228     0.0    12.0        4312           0.0   \n",
       "17088  4294958358   260520016  2508.0  2508.0       25986           0.0   \n",
       "\n",
       "       lpd_vote  gpd_vote  lrda_vote  grda_vote  ...  RP_18.16_min_20s  \\\n",
       "0      0.000000      0.25   0.000000   0.166667  ...         -0.034223   \n",
       "1      0.857143      0.00   0.071429   0.000000  ...         -0.034279   \n",
       "2      0.000000      0.00   0.000000   0.000000  ...         -0.034270   \n",
       "3      0.000000      1.00   0.000000   0.000000  ...         -0.034267   \n",
       "4      0.000000      0.00   0.000000   0.000000  ...         -0.034239   \n",
       "...         ...       ...        ...        ...  ...               ...   \n",
       "17084  0.000000      0.00   0.000000   0.500000  ...         -0.034282   \n",
       "17085  0.000000      0.00   0.000000   0.500000  ...         -0.034214   \n",
       "17086  0.000000      0.00   0.000000   0.000000  ...         -0.034285   \n",
       "17087  0.000000      0.00   0.000000   0.066667  ...         -0.034279   \n",
       "17088  0.000000      0.00   0.000000   0.000000  ...         -0.034282   \n",
       "\n",
       "      RP_18.36_min_20s  RP_18.55_min_20s  RP_18.75_min_20s  RP_18.95_min_20s  \\\n",
       "0            -0.034364         -0.033143         -0.033080         -0.031165   \n",
       "1            -0.034417         -0.033181         -0.033113         -0.031205   \n",
       "2            -0.034411         -0.033168         -0.033106         -0.031201   \n",
       "3            -0.034408         -0.033173         -0.033103         -0.031198   \n",
       "4            -0.034376         -0.033153         -0.033087         -0.031194   \n",
       "...                ...               ...               ...               ...   \n",
       "17084        -0.034420         -0.033183         -0.033115         -0.031206   \n",
       "17085        -0.034326         -0.033102         -0.033006         -0.031130   \n",
       "17086        -0.034424         -0.033186         -0.033117         -0.031208   \n",
       "17087        -0.034417         -0.033178         -0.033110         -0.031201   \n",
       "17088        -0.034420         -0.033183         -0.033113         -0.031205   \n",
       "\n",
       "       RP_19.14_min_20s  RP_19.34_min_20s  RP_19.53_min_20s  RP_19.73_min_20s  \\\n",
       "0             -0.025752         -0.026022         -0.026978         -0.024814   \n",
       "1             -0.025768         -0.026030         -0.026982         -0.024815   \n",
       "2             -0.025765         -0.026030         -0.026981         -0.024815   \n",
       "3             -0.025765         -0.026027         -0.026982         -0.024815   \n",
       "4             -0.025765         -0.026028         -0.026980         -0.024814   \n",
       "...                 ...               ...               ...               ...   \n",
       "17084         -0.025769         -0.026031         -0.026982         -0.024815   \n",
       "17085         -0.025725         -0.026012         -0.026975         -0.024812   \n",
       "17086         -0.025770         -0.026032         -0.026983         -0.024815   \n",
       "17087         -0.025767         -0.026030         -0.026982         -0.024815   \n",
       "17088         -0.025768         -0.026031         -0.026982         -0.024815   \n",
       "\n",
       "       RP_19.92_min_20s  \n",
       "0             -0.026100  \n",
       "1             -0.026101  \n",
       "2             -0.026101  \n",
       "3             -0.026101  \n",
       "4             -0.026101  \n",
       "...                 ...  \n",
       "17084         -0.026101  \n",
       "17085         -0.026098  \n",
       "17086         -0.026101  \n",
       "17087         -0.026101  \n",
       "17088         -0.026101  \n",
       "\n",
       "[17089 rows x 1612 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns to be excluded from scaling\n",
    "excluded_columns = ['eeg_id', 'spec_id', 'min', 'max', 'patient_id', 'seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote','target']\n",
    "\n",
    "# Save the columns to be excluded\n",
    "excluded_data = train[excluded_columns]\n",
    "\n",
    "# DataFrame with only the columns to be scaled\n",
    "features = train.drop(columns=excluded_columns)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform them\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Create a DataFrame from the scaled features\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "# Concatenate the scaled features with the excluded columns\n",
    "train_scaled_df = pd.concat([excluded_data.reset_index(drop=True),features_scaled_df,], axis=1)\n",
    "train_scaled_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17089 entries, 0 to 17088\n",
      "Columns: 1612 entries, eeg_id to RP_19.92_min_20s\n",
      "dtypes: float64(1608), int64(3), object(1)\n",
      "memory usage: 210.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_scaled_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>8 |</span></b> <b>TRAIN MODEL</b></div>\n",
    "\n",
    "* Original work uses catboost, let's try with XGBoost in this version to see the difference in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version 2.0.2\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import gc\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "\n",
    "print('XGBoost version', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### train size 13671, valid size 3418\n",
      "#########################\n",
      "[0]\tvalidation_0-mlogloss:1.71525\n",
      "[1]\tvalidation_0-mlogloss:1.65074\n",
      "[2]\tvalidation_0-mlogloss:1.59639\n",
      "[3]\tvalidation_0-mlogloss:1.54921\n",
      "[4]\tvalidation_0-mlogloss:1.51127\n",
      "[5]\tvalidation_0-mlogloss:1.47471\n",
      "[6]\tvalidation_0-mlogloss:1.44130\n",
      "[7]\tvalidation_0-mlogloss:1.41417\n",
      "[8]\tvalidation_0-mlogloss:1.38801\n",
      "[9]\tvalidation_0-mlogloss:1.36416\n",
      "[10]\tvalidation_0-mlogloss:1.34616\n",
      "[11]\tvalidation_0-mlogloss:1.32763\n",
      "[12]\tvalidation_0-mlogloss:1.30994\n",
      "[13]\tvalidation_0-mlogloss:1.29378\n",
      "[14]\tvalidation_0-mlogloss:1.28079\n",
      "[15]\tvalidation_0-mlogloss:1.26874\n",
      "[16]\tvalidation_0-mlogloss:1.25603\n",
      "[17]\tvalidation_0-mlogloss:1.24528\n",
      "[18]\tvalidation_0-mlogloss:1.23556\n",
      "[19]\tvalidation_0-mlogloss:1.22740\n",
      "[20]\tvalidation_0-mlogloss:1.22049\n",
      "[21]\tvalidation_0-mlogloss:1.21151\n",
      "[22]\tvalidation_0-mlogloss:1.20529\n",
      "[23]\tvalidation_0-mlogloss:1.19835\n",
      "[24]\tvalidation_0-mlogloss:1.19216\n",
      "[25]\tvalidation_0-mlogloss:1.18574\n",
      "[26]\tvalidation_0-mlogloss:1.18054\n",
      "[27]\tvalidation_0-mlogloss:1.17523\n",
      "[28]\tvalidation_0-mlogloss:1.17033\n",
      "[29]\tvalidation_0-mlogloss:1.16654\n",
      "[30]\tvalidation_0-mlogloss:1.16318\n",
      "[31]\tvalidation_0-mlogloss:1.15975\n",
      "[32]\tvalidation_0-mlogloss:1.15740\n",
      "[33]\tvalidation_0-mlogloss:1.15332\n",
      "[34]\tvalidation_0-mlogloss:1.14940\n",
      "[35]\tvalidation_0-mlogloss:1.14631\n",
      "[36]\tvalidation_0-mlogloss:1.14285\n",
      "[37]\tvalidation_0-mlogloss:1.13870\n",
      "[38]\tvalidation_0-mlogloss:1.13530\n",
      "[39]\tvalidation_0-mlogloss:1.13299\n",
      "[40]\tvalidation_0-mlogloss:1.13154\n",
      "[41]\tvalidation_0-mlogloss:1.12880\n",
      "[42]\tvalidation_0-mlogloss:1.12738\n",
      "[43]\tvalidation_0-mlogloss:1.12534\n",
      "[44]\tvalidation_0-mlogloss:1.12333\n",
      "[45]\tvalidation_0-mlogloss:1.12275\n",
      "[46]\tvalidation_0-mlogloss:1.12118\n",
      "[47]\tvalidation_0-mlogloss:1.11920\n",
      "[48]\tvalidation_0-mlogloss:1.11895\n",
      "[49]\tvalidation_0-mlogloss:1.11769\n",
      "[50]\tvalidation_0-mlogloss:1.11546\n",
      "[51]\tvalidation_0-mlogloss:1.11431\n",
      "[52]\tvalidation_0-mlogloss:1.11393\n",
      "[53]\tvalidation_0-mlogloss:1.11331\n",
      "[54]\tvalidation_0-mlogloss:1.11137\n",
      "[55]\tvalidation_0-mlogloss:1.11102\n",
      "[56]\tvalidation_0-mlogloss:1.11077\n",
      "[57]\tvalidation_0-mlogloss:1.11043\n",
      "[58]\tvalidation_0-mlogloss:1.11056\n",
      "[59]\tvalidation_0-mlogloss:1.11022\n",
      "[60]\tvalidation_0-mlogloss:1.10929\n",
      "[61]\tvalidation_0-mlogloss:1.10975\n",
      "[62]\tvalidation_0-mlogloss:1.10954\n",
      "[63]\tvalidation_0-mlogloss:1.10831\n",
      "[64]\tvalidation_0-mlogloss:1.10797\n",
      "[65]\tvalidation_0-mlogloss:1.10802\n",
      "[66]\tvalidation_0-mlogloss:1.10836\n",
      "[67]\tvalidation_0-mlogloss:1.10873\n",
      "[68]\tvalidation_0-mlogloss:1.10929\n",
      "[69]\tvalidation_0-mlogloss:1.10830\n",
      "[70]\tvalidation_0-mlogloss:1.10933\n",
      "[71]\tvalidation_0-mlogloss:1.10820\n",
      "[72]\tvalidation_0-mlogloss:1.10947\n",
      "[73]\tvalidation_0-mlogloss:1.10979\n",
      "[74]\tvalidation_0-mlogloss:1.11000\n",
      "#########################\n",
      "### Fold 2\n",
      "### train size 13671, valid size 3418\n",
      "#########################\n",
      "[0]\tvalidation_0-mlogloss:1.71541\n",
      "[1]\tvalidation_0-mlogloss:1.64985\n",
      "[2]\tvalidation_0-mlogloss:1.60134\n",
      "[3]\tvalidation_0-mlogloss:1.55657\n",
      "[4]\tvalidation_0-mlogloss:1.51656\n",
      "[5]\tvalidation_0-mlogloss:1.48017\n",
      "[6]\tvalidation_0-mlogloss:1.44887\n",
      "[7]\tvalidation_0-mlogloss:1.41938\n",
      "[8]\tvalidation_0-mlogloss:1.39499\n",
      "[9]\tvalidation_0-mlogloss:1.37213\n",
      "[10]\tvalidation_0-mlogloss:1.35371\n",
      "[11]\tvalidation_0-mlogloss:1.33732\n",
      "[12]\tvalidation_0-mlogloss:1.32096\n",
      "[13]\tvalidation_0-mlogloss:1.30671\n",
      "[14]\tvalidation_0-mlogloss:1.29306\n",
      "[15]\tvalidation_0-mlogloss:1.28102\n",
      "[16]\tvalidation_0-mlogloss:1.26967\n",
      "[17]\tvalidation_0-mlogloss:1.25941\n",
      "[18]\tvalidation_0-mlogloss:1.25155\n",
      "[19]\tvalidation_0-mlogloss:1.24309\n",
      "[20]\tvalidation_0-mlogloss:1.23450\n",
      "[21]\tvalidation_0-mlogloss:1.22792\n",
      "[22]\tvalidation_0-mlogloss:1.22093\n",
      "[23]\tvalidation_0-mlogloss:1.21584\n",
      "[24]\tvalidation_0-mlogloss:1.20992\n",
      "[25]\tvalidation_0-mlogloss:1.20597\n",
      "[26]\tvalidation_0-mlogloss:1.20263\n",
      "[27]\tvalidation_0-mlogloss:1.19660\n",
      "[28]\tvalidation_0-mlogloss:1.19221\n",
      "[29]\tvalidation_0-mlogloss:1.18722\n",
      "[30]\tvalidation_0-mlogloss:1.18416\n",
      "[31]\tvalidation_0-mlogloss:1.18224\n",
      "[32]\tvalidation_0-mlogloss:1.18006\n",
      "[33]\tvalidation_0-mlogloss:1.17728\n",
      "[34]\tvalidation_0-mlogloss:1.17452\n",
      "[35]\tvalidation_0-mlogloss:1.17281\n",
      "[36]\tvalidation_0-mlogloss:1.17022\n",
      "[37]\tvalidation_0-mlogloss:1.16821\n",
      "[38]\tvalidation_0-mlogloss:1.16795\n",
      "[39]\tvalidation_0-mlogloss:1.16729\n",
      "[40]\tvalidation_0-mlogloss:1.16625\n",
      "[41]\tvalidation_0-mlogloss:1.16422\n",
      "[42]\tvalidation_0-mlogloss:1.16199\n",
      "[43]\tvalidation_0-mlogloss:1.16143\n",
      "[44]\tvalidation_0-mlogloss:1.15867\n",
      "[45]\tvalidation_0-mlogloss:1.15756\n",
      "[46]\tvalidation_0-mlogloss:1.15652\n",
      "[47]\tvalidation_0-mlogloss:1.15459\n",
      "[48]\tvalidation_0-mlogloss:1.15242\n",
      "[49]\tvalidation_0-mlogloss:1.15093\n",
      "[50]\tvalidation_0-mlogloss:1.15022\n",
      "[51]\tvalidation_0-mlogloss:1.15026\n",
      "[52]\tvalidation_0-mlogloss:1.15092\n",
      "[53]\tvalidation_0-mlogloss:1.14967\n",
      "[54]\tvalidation_0-mlogloss:1.14920\n",
      "[55]\tvalidation_0-mlogloss:1.14874\n",
      "[56]\tvalidation_0-mlogloss:1.14809\n",
      "[57]\tvalidation_0-mlogloss:1.14686\n",
      "[58]\tvalidation_0-mlogloss:1.14705\n",
      "[59]\tvalidation_0-mlogloss:1.14580\n",
      "[60]\tvalidation_0-mlogloss:1.14502\n",
      "[61]\tvalidation_0-mlogloss:1.14496\n",
      "[62]\tvalidation_0-mlogloss:1.14530\n",
      "[63]\tvalidation_0-mlogloss:1.14456\n",
      "[64]\tvalidation_0-mlogloss:1.14360\n",
      "[65]\tvalidation_0-mlogloss:1.14320\n",
      "[66]\tvalidation_0-mlogloss:1.14455\n",
      "[67]\tvalidation_0-mlogloss:1.14405\n",
      "[68]\tvalidation_0-mlogloss:1.14406\n",
      "[69]\tvalidation_0-mlogloss:1.14428\n",
      "[70]\tvalidation_0-mlogloss:1.14311\n",
      "[71]\tvalidation_0-mlogloss:1.14363\n",
      "[72]\tvalidation_0-mlogloss:1.14341\n",
      "[73]\tvalidation_0-mlogloss:1.14424\n",
      "[74]\tvalidation_0-mlogloss:1.14413\n",
      "[75]\tvalidation_0-mlogloss:1.14379\n",
      "[76]\tvalidation_0-mlogloss:1.14448\n",
      "[77]\tvalidation_0-mlogloss:1.14464\n",
      "[78]\tvalidation_0-mlogloss:1.14446\n",
      "[79]\tvalidation_0-mlogloss:1.14388\n",
      "[80]\tvalidation_0-mlogloss:1.14434\n",
      "#########################\n",
      "### Fold 3\n",
      "### train size 13671, valid size 3418\n",
      "#########################\n",
      "[0]\tvalidation_0-mlogloss:1.70614\n",
      "[1]\tvalidation_0-mlogloss:1.63809\n",
      "[2]\tvalidation_0-mlogloss:1.58101\n",
      "[3]\tvalidation_0-mlogloss:1.53417\n",
      "[4]\tvalidation_0-mlogloss:1.49093\n",
      "[5]\tvalidation_0-mlogloss:1.45479\n",
      "[6]\tvalidation_0-mlogloss:1.42071\n",
      "[7]\tvalidation_0-mlogloss:1.39058\n",
      "[8]\tvalidation_0-mlogloss:1.36312\n",
      "[9]\tvalidation_0-mlogloss:1.33868\n",
      "[10]\tvalidation_0-mlogloss:1.31763\n",
      "[11]\tvalidation_0-mlogloss:1.29876\n",
      "[12]\tvalidation_0-mlogloss:1.28038\n",
      "[13]\tvalidation_0-mlogloss:1.26369\n",
      "[14]\tvalidation_0-mlogloss:1.25013\n",
      "[15]\tvalidation_0-mlogloss:1.23521\n",
      "[16]\tvalidation_0-mlogloss:1.22163\n",
      "[17]\tvalidation_0-mlogloss:1.20964\n",
      "[18]\tvalidation_0-mlogloss:1.19685\n",
      "[19]\tvalidation_0-mlogloss:1.18656\n",
      "[20]\tvalidation_0-mlogloss:1.17692\n",
      "[21]\tvalidation_0-mlogloss:1.16813\n",
      "[22]\tvalidation_0-mlogloss:1.15963\n",
      "[23]\tvalidation_0-mlogloss:1.15032\n",
      "[24]\tvalidation_0-mlogloss:1.14512\n",
      "[25]\tvalidation_0-mlogloss:1.13794\n",
      "[26]\tvalidation_0-mlogloss:1.13163\n",
      "[27]\tvalidation_0-mlogloss:1.12585\n",
      "[28]\tvalidation_0-mlogloss:1.11900\n",
      "[29]\tvalidation_0-mlogloss:1.11357\n",
      "[30]\tvalidation_0-mlogloss:1.10860\n",
      "[31]\tvalidation_0-mlogloss:1.10528\n",
      "[32]\tvalidation_0-mlogloss:1.09929\n",
      "[33]\tvalidation_0-mlogloss:1.09507\n",
      "[34]\tvalidation_0-mlogloss:1.09102\n",
      "[35]\tvalidation_0-mlogloss:1.08833\n",
      "[36]\tvalidation_0-mlogloss:1.08409\n",
      "[37]\tvalidation_0-mlogloss:1.08169\n",
      "[38]\tvalidation_0-mlogloss:1.07803\n",
      "[39]\tvalidation_0-mlogloss:1.07529\n",
      "[40]\tvalidation_0-mlogloss:1.07253\n",
      "[41]\tvalidation_0-mlogloss:1.07068\n",
      "[42]\tvalidation_0-mlogloss:1.06764\n",
      "[43]\tvalidation_0-mlogloss:1.06683\n",
      "[44]\tvalidation_0-mlogloss:1.06439\n",
      "[45]\tvalidation_0-mlogloss:1.06377\n",
      "[46]\tvalidation_0-mlogloss:1.06142\n",
      "[47]\tvalidation_0-mlogloss:1.05997\n",
      "[48]\tvalidation_0-mlogloss:1.05807\n",
      "[49]\tvalidation_0-mlogloss:1.05548\n",
      "[50]\tvalidation_0-mlogloss:1.05370\n",
      "[51]\tvalidation_0-mlogloss:1.05154\n",
      "[52]\tvalidation_0-mlogloss:1.04968\n",
      "[53]\tvalidation_0-mlogloss:1.04905\n",
      "[54]\tvalidation_0-mlogloss:1.04820\n",
      "[55]\tvalidation_0-mlogloss:1.04752\n",
      "[56]\tvalidation_0-mlogloss:1.04596\n",
      "[57]\tvalidation_0-mlogloss:1.04526\n",
      "[58]\tvalidation_0-mlogloss:1.04385\n",
      "[59]\tvalidation_0-mlogloss:1.04299\n",
      "[60]\tvalidation_0-mlogloss:1.04263\n",
      "[61]\tvalidation_0-mlogloss:1.04044\n",
      "[62]\tvalidation_0-mlogloss:1.03985\n",
      "[63]\tvalidation_0-mlogloss:1.04032\n",
      "[64]\tvalidation_0-mlogloss:1.03873\n",
      "[65]\tvalidation_0-mlogloss:1.03739\n",
      "[66]\tvalidation_0-mlogloss:1.03613\n",
      "[67]\tvalidation_0-mlogloss:1.03588\n",
      "[68]\tvalidation_0-mlogloss:1.03558\n",
      "[69]\tvalidation_0-mlogloss:1.03480\n",
      "[70]\tvalidation_0-mlogloss:1.03452\n",
      "[71]\tvalidation_0-mlogloss:1.03454\n",
      "[72]\tvalidation_0-mlogloss:1.03314\n",
      "[73]\tvalidation_0-mlogloss:1.03202\n",
      "[74]\tvalidation_0-mlogloss:1.03160\n",
      "[75]\tvalidation_0-mlogloss:1.03067\n",
      "[76]\tvalidation_0-mlogloss:1.03046\n",
      "[77]\tvalidation_0-mlogloss:1.02929\n",
      "[78]\tvalidation_0-mlogloss:1.02909\n",
      "[79]\tvalidation_0-mlogloss:1.02852\n",
      "[80]\tvalidation_0-mlogloss:1.02731\n",
      "[81]\tvalidation_0-mlogloss:1.02714\n",
      "[82]\tvalidation_0-mlogloss:1.02729\n",
      "[83]\tvalidation_0-mlogloss:1.02679\n",
      "[84]\tvalidation_0-mlogloss:1.02616\n",
      "[85]\tvalidation_0-mlogloss:1.02582\n",
      "[86]\tvalidation_0-mlogloss:1.02609\n",
      "[87]\tvalidation_0-mlogloss:1.02512\n",
      "[88]\tvalidation_0-mlogloss:1.02519\n",
      "[89]\tvalidation_0-mlogloss:1.02486\n",
      "[90]\tvalidation_0-mlogloss:1.02498\n",
      "[91]\tvalidation_0-mlogloss:1.02401\n",
      "[92]\tvalidation_0-mlogloss:1.02352\n",
      "[93]\tvalidation_0-mlogloss:1.02290\n",
      "[94]\tvalidation_0-mlogloss:1.02314\n",
      "[95]\tvalidation_0-mlogloss:1.02278\n",
      "[96]\tvalidation_0-mlogloss:1.02234\n",
      "[97]\tvalidation_0-mlogloss:1.02233\n",
      "[98]\tvalidation_0-mlogloss:1.02146\n",
      "[99]\tvalidation_0-mlogloss:1.02130\n",
      "#########################\n",
      "### Fold 4\n",
      "### train size 13671, valid size 3418\n",
      "#########################\n",
      "[0]\tvalidation_0-mlogloss:1.71702\n",
      "[1]\tvalidation_0-mlogloss:1.65566\n",
      "[2]\tvalidation_0-mlogloss:1.59891\n",
      "[3]\tvalidation_0-mlogloss:1.55090\n",
      "[4]\tvalidation_0-mlogloss:1.51015\n",
      "[5]\tvalidation_0-mlogloss:1.47412\n",
      "[6]\tvalidation_0-mlogloss:1.44119\n",
      "[7]\tvalidation_0-mlogloss:1.41288\n",
      "[8]\tvalidation_0-mlogloss:1.38813\n",
      "[9]\tvalidation_0-mlogloss:1.36575\n",
      "[10]\tvalidation_0-mlogloss:1.34476\n",
      "[11]\tvalidation_0-mlogloss:1.32308\n",
      "[12]\tvalidation_0-mlogloss:1.30642\n",
      "[13]\tvalidation_0-mlogloss:1.29178\n",
      "[14]\tvalidation_0-mlogloss:1.27766\n",
      "[15]\tvalidation_0-mlogloss:1.26529\n",
      "[16]\tvalidation_0-mlogloss:1.25151\n",
      "[17]\tvalidation_0-mlogloss:1.23890\n",
      "[18]\tvalidation_0-mlogloss:1.22859\n",
      "[19]\tvalidation_0-mlogloss:1.21935\n",
      "[20]\tvalidation_0-mlogloss:1.21008\n",
      "[21]\tvalidation_0-mlogloss:1.20006\n",
      "[22]\tvalidation_0-mlogloss:1.19271\n",
      "[23]\tvalidation_0-mlogloss:1.18431\n",
      "[24]\tvalidation_0-mlogloss:1.17578\n",
      "[25]\tvalidation_0-mlogloss:1.16874\n",
      "[26]\tvalidation_0-mlogloss:1.16274\n",
      "[27]\tvalidation_0-mlogloss:1.15661\n",
      "[28]\tvalidation_0-mlogloss:1.15123\n",
      "[29]\tvalidation_0-mlogloss:1.14871\n",
      "[30]\tvalidation_0-mlogloss:1.14564\n",
      "[31]\tvalidation_0-mlogloss:1.14196\n",
      "[32]\tvalidation_0-mlogloss:1.13806\n",
      "[33]\tvalidation_0-mlogloss:1.13393\n",
      "[34]\tvalidation_0-mlogloss:1.12883\n",
      "[35]\tvalidation_0-mlogloss:1.12400\n",
      "[36]\tvalidation_0-mlogloss:1.12029\n",
      "[37]\tvalidation_0-mlogloss:1.11722\n",
      "[38]\tvalidation_0-mlogloss:1.11455\n",
      "[39]\tvalidation_0-mlogloss:1.11157\n",
      "[40]\tvalidation_0-mlogloss:1.11114\n",
      "[41]\tvalidation_0-mlogloss:1.10909\n",
      "[42]\tvalidation_0-mlogloss:1.10740\n",
      "[43]\tvalidation_0-mlogloss:1.10609\n",
      "[44]\tvalidation_0-mlogloss:1.10415\n",
      "[45]\tvalidation_0-mlogloss:1.10287\n",
      "[46]\tvalidation_0-mlogloss:1.10098\n",
      "[47]\tvalidation_0-mlogloss:1.09971\n",
      "[48]\tvalidation_0-mlogloss:1.09843\n",
      "[49]\tvalidation_0-mlogloss:1.09646\n",
      "[50]\tvalidation_0-mlogloss:1.09541\n",
      "[51]\tvalidation_0-mlogloss:1.09408\n",
      "[52]\tvalidation_0-mlogloss:1.09285\n",
      "[53]\tvalidation_0-mlogloss:1.09085\n",
      "[54]\tvalidation_0-mlogloss:1.08980\n",
      "[55]\tvalidation_0-mlogloss:1.08868\n",
      "[56]\tvalidation_0-mlogloss:1.08763\n",
      "[57]\tvalidation_0-mlogloss:1.08586\n",
      "[58]\tvalidation_0-mlogloss:1.08453\n",
      "[59]\tvalidation_0-mlogloss:1.08438\n",
      "[60]\tvalidation_0-mlogloss:1.08359\n",
      "[61]\tvalidation_0-mlogloss:1.08161\n",
      "[62]\tvalidation_0-mlogloss:1.08012\n",
      "[63]\tvalidation_0-mlogloss:1.07992\n",
      "[64]\tvalidation_0-mlogloss:1.07884\n",
      "[65]\tvalidation_0-mlogloss:1.07861\n",
      "[66]\tvalidation_0-mlogloss:1.07824\n",
      "[67]\tvalidation_0-mlogloss:1.07751\n",
      "[68]\tvalidation_0-mlogloss:1.07708\n",
      "[69]\tvalidation_0-mlogloss:1.07607\n",
      "[70]\tvalidation_0-mlogloss:1.07557\n",
      "[71]\tvalidation_0-mlogloss:1.07527\n",
      "[72]\tvalidation_0-mlogloss:1.07398\n",
      "[73]\tvalidation_0-mlogloss:1.07346\n",
      "[74]\tvalidation_0-mlogloss:1.07321\n",
      "[75]\tvalidation_0-mlogloss:1.07201\n",
      "[76]\tvalidation_0-mlogloss:1.07154\n",
      "[77]\tvalidation_0-mlogloss:1.07046\n",
      "[78]\tvalidation_0-mlogloss:1.07068\n",
      "[79]\tvalidation_0-mlogloss:1.07027\n",
      "[80]\tvalidation_0-mlogloss:1.06986\n",
      "[81]\tvalidation_0-mlogloss:1.06954\n",
      "[82]\tvalidation_0-mlogloss:1.06969\n",
      "[83]\tvalidation_0-mlogloss:1.07018\n",
      "[84]\tvalidation_0-mlogloss:1.06976\n",
      "[85]\tvalidation_0-mlogloss:1.06939\n",
      "[86]\tvalidation_0-mlogloss:1.06956\n",
      "[87]\tvalidation_0-mlogloss:1.06946\n",
      "[88]\tvalidation_0-mlogloss:1.06847\n",
      "[89]\tvalidation_0-mlogloss:1.06795\n",
      "[90]\tvalidation_0-mlogloss:1.06807\n",
      "[91]\tvalidation_0-mlogloss:1.06714\n",
      "[92]\tvalidation_0-mlogloss:1.06703\n",
      "[93]\tvalidation_0-mlogloss:1.06746\n",
      "[94]\tvalidation_0-mlogloss:1.06752\n",
      "[95]\tvalidation_0-mlogloss:1.06748\n",
      "[96]\tvalidation_0-mlogloss:1.06785\n",
      "[97]\tvalidation_0-mlogloss:1.06801\n",
      "[98]\tvalidation_0-mlogloss:1.06812\n",
      "[99]\tvalidation_0-mlogloss:1.06797\n",
      "#########################\n",
      "### Fold 5\n",
      "### train size 13672, valid size 3417\n",
      "#########################\n",
      "[0]\tvalidation_0-mlogloss:1.71462\n",
      "[1]\tvalidation_0-mlogloss:1.64884\n",
      "[2]\tvalidation_0-mlogloss:1.59219\n",
      "[3]\tvalidation_0-mlogloss:1.54619\n",
      "[4]\tvalidation_0-mlogloss:1.50536\n",
      "[5]\tvalidation_0-mlogloss:1.46689\n",
      "[6]\tvalidation_0-mlogloss:1.43513\n",
      "[7]\tvalidation_0-mlogloss:1.40905\n",
      "[8]\tvalidation_0-mlogloss:1.38318\n",
      "[9]\tvalidation_0-mlogloss:1.36058\n",
      "[10]\tvalidation_0-mlogloss:1.34274\n",
      "[11]\tvalidation_0-mlogloss:1.32623\n",
      "[12]\tvalidation_0-mlogloss:1.31105\n",
      "[13]\tvalidation_0-mlogloss:1.29531\n",
      "[14]\tvalidation_0-mlogloss:1.28218\n",
      "[15]\tvalidation_0-mlogloss:1.27061\n",
      "[16]\tvalidation_0-mlogloss:1.26014\n",
      "[17]\tvalidation_0-mlogloss:1.25026\n",
      "[18]\tvalidation_0-mlogloss:1.23929\n",
      "[19]\tvalidation_0-mlogloss:1.22989\n",
      "[20]\tvalidation_0-mlogloss:1.22360\n",
      "[21]\tvalidation_0-mlogloss:1.21666\n",
      "[22]\tvalidation_0-mlogloss:1.20980\n",
      "[23]\tvalidation_0-mlogloss:1.20424\n",
      "[24]\tvalidation_0-mlogloss:1.19990\n",
      "[25]\tvalidation_0-mlogloss:1.19512\n",
      "[26]\tvalidation_0-mlogloss:1.19066\n",
      "[27]\tvalidation_0-mlogloss:1.18701\n",
      "[28]\tvalidation_0-mlogloss:1.18329\n",
      "[29]\tvalidation_0-mlogloss:1.17981\n",
      "[30]\tvalidation_0-mlogloss:1.17648\n",
      "[31]\tvalidation_0-mlogloss:1.17423\n",
      "[32]\tvalidation_0-mlogloss:1.17087\n",
      "[33]\tvalidation_0-mlogloss:1.16829\n",
      "[34]\tvalidation_0-mlogloss:1.16655\n",
      "[35]\tvalidation_0-mlogloss:1.16428\n",
      "[36]\tvalidation_0-mlogloss:1.16282\n",
      "[37]\tvalidation_0-mlogloss:1.16027\n",
      "[38]\tvalidation_0-mlogloss:1.15842\n",
      "[39]\tvalidation_0-mlogloss:1.15738\n",
      "[40]\tvalidation_0-mlogloss:1.15591\n",
      "[41]\tvalidation_0-mlogloss:1.15452\n",
      "[42]\tvalidation_0-mlogloss:1.15266\n",
      "[43]\tvalidation_0-mlogloss:1.15188\n",
      "[44]\tvalidation_0-mlogloss:1.15065\n",
      "[45]\tvalidation_0-mlogloss:1.15026\n",
      "[46]\tvalidation_0-mlogloss:1.14939\n",
      "[47]\tvalidation_0-mlogloss:1.14884\n",
      "[48]\tvalidation_0-mlogloss:1.14834\n",
      "[49]\tvalidation_0-mlogloss:1.14893\n",
      "[50]\tvalidation_0-mlogloss:1.14674\n",
      "[51]\tvalidation_0-mlogloss:1.14663\n",
      "[52]\tvalidation_0-mlogloss:1.14447\n",
      "[53]\tvalidation_0-mlogloss:1.14531\n",
      "[54]\tvalidation_0-mlogloss:1.14515\n",
      "[55]\tvalidation_0-mlogloss:1.14342\n",
      "[56]\tvalidation_0-mlogloss:1.14253\n",
      "[57]\tvalidation_0-mlogloss:1.14083\n",
      "[58]\tvalidation_0-mlogloss:1.14128\n",
      "[59]\tvalidation_0-mlogloss:1.14137\n",
      "[60]\tvalidation_0-mlogloss:1.13938\n",
      "[61]\tvalidation_0-mlogloss:1.14053\n",
      "[62]\tvalidation_0-mlogloss:1.13983\n",
      "[63]\tvalidation_0-mlogloss:1.13908\n",
      "[64]\tvalidation_0-mlogloss:1.13923\n",
      "[65]\tvalidation_0-mlogloss:1.13808\n",
      "[66]\tvalidation_0-mlogloss:1.13826\n",
      "[67]\tvalidation_0-mlogloss:1.13550\n",
      "[68]\tvalidation_0-mlogloss:1.13467\n",
      "[69]\tvalidation_0-mlogloss:1.13422\n",
      "[70]\tvalidation_0-mlogloss:1.13479\n",
      "[71]\tvalidation_0-mlogloss:1.13442\n",
      "[72]\tvalidation_0-mlogloss:1.13459\n",
      "[73]\tvalidation_0-mlogloss:1.13479\n",
      "[74]\tvalidation_0-mlogloss:1.13383\n",
      "[75]\tvalidation_0-mlogloss:1.13436\n",
      "[76]\tvalidation_0-mlogloss:1.13389\n",
      "[77]\tvalidation_0-mlogloss:1.13339\n",
      "[78]\tvalidation_0-mlogloss:1.13333\n",
      "[79]\tvalidation_0-mlogloss:1.13241\n",
      "[80]\tvalidation_0-mlogloss:1.13157\n",
      "[81]\tvalidation_0-mlogloss:1.12986\n",
      "[82]\tvalidation_0-mlogloss:1.12964\n",
      "[83]\tvalidation_0-mlogloss:1.13067\n",
      "[84]\tvalidation_0-mlogloss:1.13006\n",
      "[85]\tvalidation_0-mlogloss:1.13013\n",
      "[86]\tvalidation_0-mlogloss:1.12943\n",
      "[87]\tvalidation_0-mlogloss:1.12982\n",
      "[88]\tvalidation_0-mlogloss:1.12960\n",
      "[89]\tvalidation_0-mlogloss:1.13003\n",
      "[90]\tvalidation_0-mlogloss:1.13087\n",
      "[91]\tvalidation_0-mlogloss:1.13101\n",
      "[92]\tvalidation_0-mlogloss:1.13056\n",
      "[93]\tvalidation_0-mlogloss:1.13073\n",
      "[94]\tvalidation_0-mlogloss:1.13065\n",
      "[95]\tvalidation_0-mlogloss:1.13081\n",
      "[96]\tvalidation_0-mlogloss:1.13056\n"
     ]
    }
   ],
   "source": [
    "all_oof = []\n",
    "all_true = []\n",
    "TARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "for i, (train_index, valid_index) in enumerate(gkf.split(train , train .target, train .patient_id)):   \n",
    "    \n",
    "    print('#'*25)\n",
    "    print(f'### Fold {i+1}')\n",
    "    print(f'### train size {len(train_index)}, valid size {len(valid_index)}')\n",
    "    print('#'*25)\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob', \n",
    "        num_class=len(TARS),\n",
    "        learning_rate = 0.1, \n",
    "                      \n",
    "#         tree_method='gpu_hist',  #skip GPU acceleration\n",
    "    )\n",
    "    \n",
    "    # Prepare training and validation data\n",
    "    X_train = train.loc[train_index, FEATURES]\n",
    "    y_train = train.loc[train_index, 'target'].map(TARS)\n",
    "    X_valid = train.loc[valid_index, FEATURES]\n",
    "    y_valid = train.loc[valid_index, 'target'].map(TARS)\n",
    "    \n",
    "    model.fit(X_train, y_train, \n",
    "              eval_set=[(X_valid, y_valid)], \n",
    "              verbose=True, \n",
    "              early_stopping_rounds=10)\n",
    "    model.save_model(f'XGB_v{VER}_f{i}.model')\n",
    "    \n",
    "    oof = model.predict_proba(X_valid)\n",
    "    all_oof.append(oof)\n",
    "    all_true.append(train.loc[valid_index, TARGETS].values)\n",
    "    \n",
    "    del X_train, y_train, X_valid, y_valid, oof\n",
    "    gc.collect()\n",
    "    \n",
    "all_oof = np.concatenate(all_oof)\n",
    "all_true = np.concatenate(all_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>9 |</span></b> <b>HYPERPARAMETER TUNING</b></div>\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 9.1 |</span> Import Libraries and Set Up Optuna</b>\n",
    "* First, you import necessary libraries: optuna for hyperparameter optimization, xgboost for the machine learning model, log_loss from scikit-learn for the evaluation metric, and GroupKFold for cross-validation.\n",
    "* optuna.create_study(direction='minimize') creates a new optimization study. The direction='minimize' means you want to minimize the value returned by the objective function, which in this case is the log loss.\n",
    "\n",
    "\n",
    "* まず、必要なライブラリをインポートします。ハイパーパラメータ最適化には optuna、機械学習モデルには xgboost、評価メトリックには scikit-learn の log_loss、相互検証には GroupKFold です。\n",
    "* optuna.create_study(direction='minimize') は新しい最適化スタディを作成します。 direct='minimize' は、目的関数によって返される値 (この場合は対数損失) を最小化することを意味します。\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 9.2 |</span> Define the Objective Function</b>\n",
    "* The objective function is what Optuna will optimize. This function takes a trial object, which is used to suggest values for the hyperparameters.\n",
    "* Inside this function, you set up the hyperparameter space. Optuna will test different combinations of these parameters:\n",
    "1. lambda, alpha: Regularization parameters.\n",
    "2. colsample_bytree, subsample: Ratios for column and row sampling.\n",
    "3. learning_rate: Step size shrinkage used to prevent overfitting.\n",
    "4. n_estimators: Number of gradient boosted trees.\n",
    "5. max_depth: Maximum depth of a tree.\n",
    "6. min_child_weight: Minimum sum of instance weight needed in a child.\n",
    "\n",
    "\n",
    "* 目的関数は Optuna が最適化するものです。 この関数は、ハイパーパラメータの値を提案するために使用されるトライアル オブジェクトを受け取ります。\n",
    "※この関数内でハイパーパラメータ空間を設定します。 Optuna は、次のパラメータのさまざまな組み合わせをテストします。\n",
    "1. ラムダ、アルファ: 正則化パラメータ。\n",
    "2.colsample_bytree、subsample: 列と行のサンプリングの比率。\n",
    "3. learning_rate: 過学習を防ぐために使用されるステップ サイズの縮小。\n",
    "4. n_estimators: 勾配ブーストされたツリーの数。\n",
    "5. max_ Depth: ツリーの最大の深さ。\n",
    "6. min_child_weight: 子に必要なインスタンスの重みの最小合計。\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 9.3 |</span> Cross-Validation Loop</b>\n",
    "\n",
    "* The function uses GroupKFold for splitting the data. This method is suitable when you have groups in your data (like patient IDs) that should not be split across the training and validation sets.\n",
    "* For each fold in the cross-validation, the function:\n",
    "1. Splits the data into training and validation sets.\n",
    "2. Trains an XGBoost model using the parameters suggested by Optuna.\n",
    "3. Computes the log loss on the validation set.\n",
    "4. The average log loss across all folds is returned. Optuna will use this value to decide which hyperparameters are best.\n",
    "\n",
    "* この関数はデータの分割にGroupKFoldを使用します。 この方法は、トレーニング セットと検証セットに分割すべきでないグループ (患者 ID など) がデータ内にある場合に適しています。\n",
    "* 相互検証の各フォールドについて、関数は次のとおりです。\n",
    "1. データをトレーニング セットと検証セットに分割します。\n",
    "2. Optuna が提案するパラメーターを使用して XGBoost モデルをトレーニングします。\n",
    "3. 検証セットのログ損失を計算します。\n",
    "4. すべてのフォールドにわたる平均対数損失が返されます。 Optuna はこの値を使用して、どのハイパーパラメータが最適かを決定します。\n",
    "\n",
    "\n",
    "### <b><span style='color:#FFCE30'> 9.4 |</span> Running the Optuna Study</b>\n",
    "\n",
    "* study.optimize(objective, n_trials=100) tells Optuna to optimize the objective function. It will try 100 different combinations of hyperparameters (n_trials=100) to find the best ones.\n",
    "* It is best to start with small trials before investing time to run on more trials to manage time invested\n",
    "* Once the optimization is complete, the best hyperparameters found are printed.\n",
    "\n",
    "* Study.optimize(objective, n_trials=100) は、Optuna に目的関数を最適化するように指示します。 ハイパーパラメータの 100 種類の異なる組み合わせ (n_trials=100) を試して、最適なものを見つけます。\n",
    "* 投資時間を管理するために、より多くのトライアルを実行するために時間を投資する前に、小規模なトライアルから始めることが最善です。\n",
    "* 最適化が完了すると、見つかった最適なハイパーパラメータが出力されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-13 18:20:57,122] A new study created in memory with name: no-name-99e933e5-802a-4f91-9de3-09c4540ee6e4\n",
      "[W 2024-03-13 18:23:46,524] Trial 0 failed with parameters: {'lambda': 0.07252433430783983, 'alpha': 7.947744467850382, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.008, 'max_depth': 13, 'min_child_weight': 160} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\takashi\\AppData\\Local\\Temp\\ipykernel_28144\\4235841052.py\", line 29, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=10)\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1515, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py\", line 181, in train\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"c:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py\", line 2050, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-13 18:23:46,563] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(cv_scores)\n\u001b[0;32m     35\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Increase n_trials for more extensive search\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of finished trials:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     26\u001b[0m y_train, y_valid \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mloc[train_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(TARS), train\u001b[38;5;241m.\u001b[39mloc[valid_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(TARS)\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_valid)\n\u001b[0;32m     31\u001b[0m cv_scores\u001b[38;5;241m.\u001b[39mappend(log_loss(y_valid, preds))\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1515\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1487\u001b[0m (\n\u001b[0;32m   1488\u001b[0m     model,\n\u001b[0;32m   1489\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1495\u001b[0m )\n\u001b[0;32m   1496\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1497\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1498\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1512\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1513\u001b[0m )\n\u001b[1;32m-> 1515\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\takashi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2049\u001b[0m     _check_call(\n\u001b[1;32m-> 2050\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2051\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2053\u001b[0m     )\n\u001b[0;32m   2054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2055\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned by Optuna\n",
    "    param = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': len(TARS),\n",
    "        'tree_method': 'gpu_hist',  # use 'gpu_hist' for GPU\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-4, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-4, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008, 0.01, 0.02, 0.05, 0.1]),\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13]),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_index, valid_index in gkf.split(train, train.target, train.patient_id):\n",
    "        X_train, X_valid = train.loc[train_index, FEATURES], train.loc[valid_index, FEATURES]\n",
    "        y_train, y_valid = train.loc[train_index, 'target'].map(TARS), train.loc[valid_index, 'target'].map(TARS)\n",
    "\n",
    "        model = xgb.XGBClassifier(**param)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False, early_stopping_rounds=10)\n",
    "        preds = model.predict_proba(X_valid)\n",
    "        cv_scores.append(log_loss(y_valid, preds))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)  # Increase n_trials for more extensive search\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>10 |</span></b> <b>FEATURE IMPORTANCE</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 30\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# Get the feature names from 'train'\n",
    "feature_names = train.columns\n",
    "\n",
    "# Sort the feature importances and get the indices of the sorted array\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "# Plot only the top 'TOP' features\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.barh(np.arange(len(sorted_idx))[-TOP:], feature_importance[sorted_idx][-TOP:], align='center')\n",
    "plt.yticks(np.arange(len(sorted_idx))[-TOP:], feature_names[sorted_idx][-TOP:])\n",
    "plt.title(f'Feature Importance - Top {TOP}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>11 |</span></b> <b>INFER TEST</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/hms-harmful-brain-activity-classification/test.csv')\n",
    "print('Test shape',test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH2 = '../input/hms-harmful-brain-activity-classification/test_spectrograms/'\n",
    "s = \"853520\"\n",
    "spec = pd.read_parquet(f'{PATH2}{s}.parquet')\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# READ ALL TEST SPECTROGRAMS\n",
    "PATH2 = '../input/hms-harmful-brain-activity-classification/test_spectrograms/'\n",
    "files = os.listdir(PATH2)\n",
    "print(f'There are {len(files)} spectrogram parquets')\n",
    "\n",
    "spectrograms = {}\n",
    "for i,f in enumerate(files):\n",
    "    if i%100==0: print(i,', ',end='')\n",
    "    tmp = pd.read_parquet(f'{PATH2}{f}')\n",
    "    name = int(f.split('.')[0])\n",
    "    spectrograms_test[name] = tmp.iloc[:,1:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# ENGINEER FEATURES\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# The code generates features from the spectrogram data for use in a model \n",
    "# The features are derived by calculating the mean and minimum values over time for each of the 400 spectrogram frequencies.\n",
    "# Two types of windows are used for these calculations:\n",
    "# A 10-minute window (_mean_10m, _min_10m).\n",
    "# A 20-second window (_mean_20s, _min_20s).\n",
    "# This process results in 1600 features (400 features × 4 calculations) for each EEG ID.\n",
    "\n",
    "SPEC_COLS = pd.read_parquet(f'{PATH}1000086677.parquet').columns[1:]\n",
    "FEATURES = [f'{c}_mean_10m' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_min_10m' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_mean_20s' for c in SPEC_COLS]\n",
    "FEATURES += [f'{c}_min_20s' for c in SPEC_COLS]\n",
    "print(f'We are creating {len(FEATURES)} features for {len(test)} rows... ',end='')\n",
    "\n",
    "\n",
    "# A data matrix data is initialized to store the new features for each eeg_id in the train DataFrame.\n",
    "# For each row in train, the code calculates the mean and minimum values within the specified 10-minute and 20-second windows.\n",
    "# These calculated values are then stored in the data matrix.\n",
    "# Finally, the matrix is added to the train DataFrame as new columns.\n",
    "\n",
    "data = np.zeros((len(test),len(FEATURES)))\n",
    "for k in range(len(test)):\n",
    "    if k%100==0: print(k,', ',end='')\n",
    "    row = test.iloc[k]\n",
    "            \n",
    "    # 10 MINUTE WINDOW FEATURES\n",
    "    x = np.nanmean( spec.iloc[:,1:].values, axis=0)\n",
    "    data[k,:400] = x\n",
    "    x = np.nanmin( spec.iloc[:,1:].values, axis=0)\n",
    "    data[k,400:800] = x\n",
    "\n",
    "    # 20 SECOND WINDOW FEATURES\n",
    "    x = np.nanmean( spec.iloc[145:155,1:].values, axis=0)\n",
    "    data[k,800:1200] = x\n",
    "    x = np.nanmin( spec.iloc[145:155,1:].values, axis=0)\n",
    "    data[k,1200:1600] = x\n",
    "\n",
    "    test[FEATURES] = data\n",
    "\n",
    "    \n",
    "print()\n",
    "print('New test shape:',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Initialize a PCA model\n",
    "pca = PCA(n_components=0.95)\n",
    "print(\"PCA model initialized.\")\n",
    "\n",
    "# Initialize an array for original features\n",
    "num_rows = len(test)\n",
    "num_features = 20 * n_channels  # 20 features per channel\n",
    "data_original = np.zeros((num_rows, num_features))\n",
    "\n",
    "print(\"Starting feature extraction and PCA processing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for k in range(num_rows):\n",
    "    if k % 1000 == 0:\n",
    "        print(f\"Processing row {k} of {num_rows}...\")\n",
    "\n",
    "    row = train.iloc[k]\n",
    "    eeg_segment = spectrograms_test[853520][r:r+300, :]\n",
    "\n",
    "    # Apply the feature extraction function to each EEG channel\n",
    "    all_channel_features = []\n",
    "    for i in range(n_channels):\n",
    "        channel_features = extract_frequency_band_features(eeg_segment[:, i])\n",
    "        all_channel_features.extend(channel_features)\n",
    "    \n",
    "    data_original[k, :] = all_channel_features\n",
    "\n",
    "print(\"Data matrix constructed\")\n",
    "\n",
    "# Impute NaN values in the data matrix\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_original)\n",
    "\n",
    "print(f\"NaN values handled. Imputed data matrix shape: {data_imputed.shape}\")\n",
    "\n",
    "# Apply PCA on the imputed data\n",
    "pca.fit(data_imputed)\n",
    "print(\"PCA fitting completed.\")\n",
    "\n",
    "# Transform data using PCA\n",
    "data_pca = pca.transform(data_imputed)\n",
    "\n",
    "# Add PCA features to DataFrame\n",
    "pca_feature_columns = [f'pca_feature_{i}' for i in range(data_pca.shape[1])]\n",
    "test[pca_feature_columns] = data_pca\n",
    "\n",
    "# Measure total processing time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total processing time: {total_time:.2f} seconds.\")\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to be excluded from scaling\n",
    "excluded_columns = ['eeg_id', 'spectrogram_id', 'patient_id']\n",
    "\n",
    "# Save the columns to be excluded\n",
    "excluded_data = test[excluded_columns]\n",
    "\n",
    "# DataFrame with only the columns to be scaled\n",
    "features = test.drop(columns=excluded_columns)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features and transform them\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Create a DataFrame from the scaled features\n",
    "features_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "# Concatenate the scaled features with the excluded columns\n",
    "test_scaled_df = pd.concat([excluded_data.reset_index(drop=True),features_scaled_df,], axis=1)\n",
    "test_scaled_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEER TEST\n",
    "PATH2 = '../input/hms-harmful-brain-activity-classification/test_spectrograms/'\n",
    "data = np.zeros((len(test),len(FEATURES)))\n",
    "    \n",
    "for k in range(len(test)):\n",
    "    row = test.iloc[k]\n",
    "    s = int( row.spectrogram_id )\n",
    "    spec = pd.read_parquet(f'{PATH2}{s}.parquet')\n",
    "    \n",
    "    # 10 MINUTE WINDOW FEATURES\n",
    "    x = np.nanmean( spec.iloc[:,1:].values, axis=0)\n",
    "    data[k,:400] = x\n",
    "    x = np.nanmin( spec.iloc[:,1:].values, axis=0)\n",
    "    data[k,400:800] = x\n",
    "\n",
    "    # 20 SECOND WINDOW FEATURES\n",
    "    x = np.nanmean( spec.iloc[145:155,1:].values, axis=0)\n",
    "    data[k,800:1200] = x\n",
    "    x = np.nanmin( spec.iloc[145:155,1:].values, axis=0)\n",
    "    data[k,1200:1600] = x\n",
    "\n",
    "test[FEATURES] = data\n",
    "print('New test shape',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFER XGBOOST ON TEST\n",
    "preds = []\n",
    "\n",
    "for i in range(5):\n",
    "    print(i, ', ', end='')\n",
    "    \n",
    "    # Load the XGBoost model\n",
    "    model = xgb.XGBClassifier()\n",
    "    model.load_model(f'XGB_v{VER}_f{i}.model')\n",
    "    \n",
    "    # Make predictions\n",
    "    pred = model.predict_proba(test[FEATURES])\n",
    "    preds.append(pred)\n",
    "\n",
    "# Average the predictions from each fold\n",
    "pred = np.mean(preds, axis=0)\n",
    "print()\n",
    "print('Test preds shape', pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"padding: 30px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#FFCE30\"><b><span style='color:#FFFFFF'>12 |</span></b> <b>SUBMISSION</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'eeg_id':test.eeg_id.values})\n",
    "sub[TARGETS] = pred\n",
    "# sub.to_csv('submission.csv',index=False)\n",
    "print('Submission shape',sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\n",
    "sub.iloc[:,-6:].sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
